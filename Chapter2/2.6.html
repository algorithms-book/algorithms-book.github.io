<!DOCTYPE html>
<html>
<head>

  <title>2.6 The Fast Fourier Transform</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <!-- KaTeX -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.css">
  <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/katex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/katex@0.10.0-beta/dist/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="the-fast-fourier-transform">2.6 The Fast Fourier Transform</h2>
  <p>We have so far seen how divide-and-conquer gives fast algorithms for multiplying integers and matrices; our next target is <em>polynomials</em>. The product of two degree-<span class="math inline">\(d\)</span> is as polynomial of degree <span class="math inline">\(2d\)</span>.</p>
  <p>For example:</p>
  <p><span class="math display">\[
  (1 + 2x + 3x^2) \cdot (2 + x + 4x^2) = 2 + 5x + 12x^2 + 11x^3 + 12x^4.
  \]</span></p>
  <p>More generally, if <span class="math inline">\(A(x) = a_0 + a_1 x + \cdots + a_d x^{d}\)</span> and <span class="math inline">\(B(x) = b_0 + b_1 x + \cdots + b_d x^{d}\)</span>, their product <span class="math inline">\(C(x) = A(x) \cdot B(x) = c_0 + c_1 x + \cdots + c_{2d} x^{2d}\)</span> has coefficients</p>
  <p><span class="math display">\[
  c_k = a_0 b_k + a_1 b_{k - 1} + \cdots + a_k b_0 = \sum_{i = 0}^{k} a_i b_{k - i}
  \]</span></p>
  <p>(for <span class="math inline">\(i &gt; d\)</span>, take <span class="math inline">\(a_i\)</span> and <span class="math inline">\(b_i\)</span> to be zero). Computing <span class="math inline">\(c_k\)</span> from this formula takes <span class="math inline">\(O(k)\)</span> steps, and finding all <span class="math inline">\(2d + 1\)</span> coefficients would therefore seem to require <span class="math inline">\(\Theta(d^2)\)</span> time.</p>
  <p><em>Can we possibly multiply polynomials faster that this?</em></p>
  <p>The solution we will develop, the fast Fourier Transform, has revolutionized—indeed, defined—the field of signal processing (sell the following box). Because of its huge importance, and its wealth of insights from different fields of study, we will approach it a little more leisurely than usual.</p>
  <p>The reader who wants just the core algorithm can skip directly to Section 2.6.4.</p>
  <p> </p>
  <blockquote>
  <p><strong>Why multiply polynomials?</strong></p>
  <p>For one thing, it turns out that the fastest algorithms we have for multiplying integers rely heavily on polynomial multiplication; after all, polynomials and binary integers are quite similar—just replace the variable <span class="math inline">\(x\)</span> by the base <span class="math inline">\(2\)</span>, and watch out for carries. But perhaps more importantly, multiplying polynomials is crucial for <em>signal processing</em>.</p>
  <p>A <em>signal</em> is any quantity that is a function of time (as in Figure (a)) or of position. It might, for instance, capture a human voice by measuring fluctuations in air pressure close to the speaker’s mouth, or alternatively, the pattern of stars in the night sky, by measuring brightness as a function of angle.</p>
  <div class="figure">
  <img src="signal-processing.png" />

  </div>
  <p>In order to extract information from a signal, we need to first <em>digitize</em> it by sampling (Figure (b))—and, then, to put it through a <em>system</em> that will transform it in some way. The output is called the <em>response</em> of the system: <span class="math display">\[ \text{signal} \ \longrightarrow \ \boxed{\text{SYSTEM}} \ \longrightarrow \ \text{response} \]</span></p>
  <p>An important class of systems are those that are <em>linear</em>—the response to the sum of two signals is just the sum of their individual responses—and <em>time invariant</em>—shifting the input signal by time <span class="math inline">\(t\)</span> produces the same output, also shifted by <span class="math inline">\(t\)</span>. Any system with these properties is completely characterized by its response to the simplest possible input signal: the <em>unit impulse</em> <span class="math inline">\(\delta(t)\)</span>, consisting solely of a &quot;jerk&quot; at <span class="math inline">\(t = 0\)</span> (Figure (c)). To see this, first consider the close relative <span class="math inline">\(\delta(t - i)\)</span>, a shifted impulse in which the jerk occurs at time <span class="math inline">\(i\)</span>. Any signal <span class="math inline">\(a(t)\)</span> can be expressed as a linear combination of these, letting <span class="math inline">\(\delta(t - i)\)</span> pick out its behavior at time <span class="math inline">\(i\)</span>, <span class="math display">\[a(t) = \sum_{i = 0}^{T - 1} a(t) \delta(t - i)\]</span> (if the signal consists of <span class="math inline">\(T\)</span> samples). By linearity, the system response to input <span class="math inline">\(a(t)\)</span> is determined by the responses to the various <span class="math inline">\(\delta(t-i)\)</span>. And by time invariance, these are in turn just shifted copies of the <em>impulse response</em> <span class="math inline">\(b(t)\)</span>, the response to <span class="math inline">\(\delta(t)\)</span>.</p>
  <p>In other words, the output of the system at time <span class="math inline">\(k\)</span> is <span class="math display">\[c(k) = \sum_{i = 0}^{k} a(i)b(k - i)\]</span> exactly the formula for polynomial multiplication!</p>
  </blockquote>
  <p> </p>
  <h3 id="an-alternative-representation-of-polynomials">2.6.1 An Alternative Representation of Polynomials</h3>
  <p>To arrive at a fast algorithm for polynomial multiplication we take inspiration from an important property of polynomials.</p>
  <p><span class="math inline">\(\textbf{Lemma}\ \)</span> A degree-<span class="math inline">\(d\)</span> polynomial is uniquely characterized by its values at any <span class="math inline">\(d + 1\)</span> distinct points.</p>
  <p>A familiar instance of this is that &quot;any two points determine a line.&quot; We will later see why the more general statement is true, but for the time being it gives us an alternative representation of polynomials.</p>
  <p>Fix any distinct points <span class="math inline">\(x_0, \cdots, x_d\)</span>. We can specify a degree-<span class="math inline">\(d\)</span> polynomial <span class="math inline">\(A(x) = a_0 + a_1 x + \cdots + a_d x^d\)</span> by either one of the following:</p>
  <ol style="list-style-type: decimal">
  <li><p>Its coefficients <span class="math inline">\(a_0, a_1, \cdots, a_d\)</span></p></li>
  <li><p>The values <span class="math inline">\(A(x_0), A(x_1), \cdots, A(x_d)\)</span></p></li>
  </ol>
  <p>Of these two representations, the second is the more attractive for polynomial multiplication. Since the product <span class="math inline">\(C(x)\)</span> has degree <span class="math inline">\(2d\)</span>, it is completely determined by its value at any <span class="math inline">\(2d + 1\)</span> points. And its value at any given point <span class="math inline">\(z\)</span> is easy enough to figure out, just <span class="math inline">\(A(z)\)</span> times <span class="math inline">\(B(z)\)</span>. Thus <em>polynomial multiplication takes linear time in the value representation</em>.</p>
  <p>The problem is that we expect the input polynomials, and also their product, to be specified by coefficients. So we need to first translate from coefficients to values—which is just a matter of <em>evaluating</em> the polynomial at the chosen points—then multiply in the value representation, and finally translate back to coefficients, a process called <em>interpolation</em>.</p>
  <div class="figure">
  <img src="evaluation-interpolation.png" />

  </div>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> poly_mult(A, B):
      <span class="co">&quot;&quot;&quot;</span>
  <span class="co">    Input: coefficients of two polynomials, A and B, of degree d</span>
  <span class="co">    Output: their product C = A * B</span>
  <span class="co">    &quot;&quot;&quot;</span>
      Selection
          pick some some points x_0, x_1, ..., x_{n <span class="op">-</span> <span class="dv">1</span>} where n <span class="op">&gt;=</span> 2d <span class="op">+</span> <span class="dv">1</span>
      Evaluation
          compute A(x_0), A(x_1), ..., A(x_{n <span class="op">-</span> <span class="dv">1</span>}) <span class="kw">and</span> B(x_0), B(x_1), ..., B(x_{n <span class="op">-</span> <span class="dv">1</span>})
      Multiplication
          compute C(x_k) <span class="op">=</span> A(x_k)B(x_k) <span class="cf">for</span> <span class="bu">all</span> k <span class="op">=</span> <span class="dv">0</span>, ..., n <span class="op">-</span> <span class="dv">1</span>
      Interpolation
          recover C(x) <span class="op">=</span> c_0 <span class="op">+</span> c_1 x <span class="op">+</span> ... <span class="op">+</span> c_{2d} x<span class="op">^</span>{2d}</code></pre></div>
  <p>The equivalence of the two polynomial representations makes it clear that this high-level approach is correct, but how efficient is it? Certainly the selection step and the <span class="math inline">\(n\)</span> multiplications are no trouble at all, just linear time.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> But (leaving aside interpolation, about which we know even less) how about evaluation? Evaluating a polynomial of degree <span class="math inline">\(d \leq n\)</span> at a single point takes <span class="math inline">\(O(n)\)</span> steps (Exercise 2.29), and so the baseline for <span class="math inline">\(n\)</span> points is <span class="math inline">\(\Theta(n^2)\)</span>. </p>
  <p>We’ll now see that the fast Fourier transform (FFT) does it in just <span class="math inline">\(O(n \log{n})\)</span> time, for a particularly clever choice of <span class="math inline">\(x_0, \cdots, x_{n-1}\)</span> in which the computations required by the individual points overlap with one another and can be shared.</p>
  <p> </p>
  <h3 id="evaluation-by-divide-and-conquer">2.6.2 Evaluation by Divide-and-Conquer</h3>
  <p>Here’s an idea for how to pick the n points at which to evaluate a polynomial <span class="math inline">\(A(x)\)</span> of degree <span class="math inline">\(\leq n - 1\)</span>. If we choose them to be positive-negative pairs, that is,</p>
  <p><span class="math display">\[
  \pm x_0, \pm x_1, \cdots, \pm x_{n / 2 - 1},
  \]</span></p>
  <p>then the computations required for each <span class="math inline">\(A(x_i)\)</span> and <span class="math inline">\(A(-x_i)\)</span> overlap a lot, because the even powers of <span class="math inline">\(x_i\)</span> coincide with those of <span class="math inline">\(-x_i\)</span>.</p>
  <p>To investigate this, we need to split <span class="math inline">\(A(x)\)</span> into its odd and even powers, for instance</p>
  <p><span class="math display">\[
  3 + 4x + 6x^2 + 2x^3 + x^4 + 10x^5 = (3 + 6x^2 + x^4) + x(4 + 2x^2 + 10x^4).
  \]</span></p>
  <p>Notice that the terms in parentheses are polynomials in <span class="math inline">\(x^2\)</span>. More generally,</p>
  <p><span class="math display">\[
  A(x) = A_{e}(x^2) + xA_{o}(x^2),
  \]</span></p>
  <p>where <span class="math inline">\(A_{e}(\cdot)\)</span>, with the even-numbered coefficients, and <span class="math inline">\(A_{o}(\cdot)\)</span>, with the odd-numbered coefficients, are polynomials of degree <span class="math inline">\(\leq n / 2 - 1\)</span> (assume for convenience that <span class="math inline">\(n\)</span> is even). Given paired points <span class="math inline">\(\pm x_i\)</span>, the calculations needed for <span class="math inline">\(A(x_i)\)</span> can be recycled toward computing <span class="math inline">\(A(-x_i)\)</span>:</p>
  <p><span class="math display">\[
  \begin{aligned}
  A(x) &amp;= A_{e}(x_i^2) + x_i A_{o}(x_i^2)
  A(-x) &amp;= A_{e}(x_i^2) - x_i A_{o}(x_i^2)
  \end{aligned}
  \]</span></p>
  <p>In other words, evaluating <span class="math inline">\(A(x)\)</span> at <span class="math inline">\(n\)</span> paired points <span class="math inline">\(\pm x_0, \cdots, \pm x_{n / 2 - 1}\)</span> reduces to evaluating <span class="math inline">\(A_{e}(x)\)</span> and <span class="math inline">\(A_{o}(x)\)</span> (which each have half the degree of <span class="math inline">\(A(x)\)</span>) at just <span class="math inline">\(n / 2\)</span> points, <span class="math inline">\(x_0^2, \cdots, x_{n / 2 - 1}^2\)</span>.</p>
  <div class="figure">
  <img src="even-odd-polynomial.png" />

  </div>
  <p>The original problem of size <span class="math inline">\(n\)</span> is in this way recast as two subproblems of size <span class="math inline">\(n / 2\)</span>, followed by some linear-time arithmetic. If we could recurse, we would get a divide-and-conquer procedure with running time</p>
  <p><span class="math display">\[
  T(n) = 2T(n / 2) + O(n),
  \]</span></p>
  <p>which is <span class="math inline">\(O(n \log{n})\)</span>, exactly what we want.</p>
  <p>But we have a problem: The plus-minus trick only works at the top level of the recursion. To recurse at the next level, we need the <span class="math inline">\(n / 2\)</span> evaluation points <span class="math inline">\(x_0^2, \cdots, x_{n / 2 - 1}^2\)</span> to be <em>themselves</em> plus-minus pairs. But how can a square be negative? The task seems impossible! Unless, of course, <em>we use complex numbers</em>.</p>
  <p>Fine, but which complex numbers? To figure this out, let us &quot;reverse engineer&quot; the process. At the very bottom of the recursion, we have a single point. This point might as well be <span class="math inline">\(1\)</span>, in which case the level above it must consist of its square roots, <span class="math inline">\(\pm \sqrt{1} = \pm 1\)</span>.</p>
  <div class="figure">
  <img src="roots-of-unity.png" />

  </div>
  <p>The next level up then has <span class="math inline">\(\pm \sqrt{+1} = \pm 1\)</span> as well as the <em>complex</em> numbers <span class="math inline">\(\pm \sqrt{-1} = \pm i\)</span>, where <span class="math inline">\(i\)</span> is the imaginary unit. By continuing in this manner, we eventually reach the initial set of <span class="math inline">\(n\)</span> points. Perhaps you have already guessed what they are: the <em>complex <span class="math inline">\(n\)</span>th roots of unity</em>, that is, the <span class="math inline">\(n\)</span> complex solutions to the equation <span class="math inline">\(z^n = 1\)</span>.</p>
  <p>Figure 2.6 is a pictorial review of some basic facts about complex numbers. The third panel of this figure introduces the <span class="math inline">\(n\)</span>th roots of unity: the complex numbers <span class="math inline">\(1, \omega, \omega^2, \cdots, \omega^{n - 1}\)</span>, where <span class="math inline">\(\omega = e^{2 \pi i / n}\)</span>. If <span class="math inline">\(n\)</span> is even,</p>
  <ol style="list-style-type: decimal">
  <li><p>The <span class="math inline">\(n\)</span>th roots are plus-minus paired, <span class="math inline">\(\omega^{n / 2 + j} = - \omega^{j}\)</span>.</p></li>
  <li><p>Squaring them produces the (<span class="math inline">\(n / 2\)</span>)nd roots of unity.</p></li>
  </ol>
  <p>Therefore, if we start with these numbers for some n that is a power of <span class="math inline">\(2\)</span>, then at successive levels of recursion we will have the (<span class="math inline">\(n / 2^k\)</span>)th roots of unity, for <span class="math inline">\(k = 0, 1, 2, 3, \cdots\)</span>. All these sets of numbers are plus-minus paired, and so our divide-and-conquer, as shown in the last panel, works perfectly. The resulting algorithm is the fast Fourier transform (Figure 2.7).</p>
  <p> </p>
  <center><div class="figure">
  <img src="fig-2.6-complex-plane.png" alt="Figure 2.6 The complex roots of unity are ideal for our divide-and-conquer scheme." />
  <p class="caption"><strong>Figure 2.6</strong> The complex roots of unity are ideal for our divide-and-conquer scheme.</p>
  </div></center>
  <p> </p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> FFT(A, w):
      <span class="co">&quot;&quot;&quot;</span>
  <span class="co">    Input: coefficient representation of a polynomial A with degree &lt;= n - 1, where n is a power of 2</span>
  <span class="co">           w, an n-th root of unity</span>
  <span class="co">    Output: value representation of A(w^{0}), ..., A(w^{n - 1})</span>
  <span class="co">    &quot;&quot;&quot;</span>
      <span class="cf">if</span> w <span class="op">==</span> <span class="dv">1</span>:
          <span class="cf">return</span> A(<span class="dv">1</span>)
      <span class="cf">else</span>:
          express A(x) <span class="kw">in</span> form of A_e(x<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span> x A_o(x<span class="op">^</span><span class="dv">2</span>)

          FFT(A_e, w<span class="op">^</span><span class="dv">2</span>) to evaluate A_e at even powers of w
          FFT(A_o, w<span class="op">^</span><span class="dv">2</span>) to evaluate A_o at even powers of w

          <span class="cf">for</span> j <span class="op">=</span> <span class="dv">0</span> to n <span class="op">-</span> <span class="dv">1</span>:
              compute A(w<span class="op">^</span>j) <span class="op">=</span> A_e(w<span class="op">^</span>{2j}) <span class="op">+</span> w<span class="op">^</span>j A_o(w<span class="op">^</span>{2j})

          <span class="cf">return</span> A(w<span class="op">^</span><span class="dv">0</span>), ..., A(w<span class="op">^</span>{n <span class="op">-</span> <span class="dv">1</span>})</code></pre></div>
  <center><p><strong>Figure 2.7</strong> The fast Fourier transform algorithm.</p></center>
  <p> </p>
  <h3 id="interpolation">2.6.3 Interpolation</h3>
  <p>Let’s take stock of where we are. We first developed a high-level scheme for multiplying polynomials (Figure 2.5), based on the observation that polynomials can be represented in two ways, in terms of their <em>coefficients</em> or in terms of their <em>values</em> at a selected set of points.</p>
  <div class="figure">
  <img src="evaluation-interpolation.png" />

  </div>
  <p>The value representation makes it trivial to multiply polynomials, but we cannot ignore the coefficient representation since it is the form in which the input and output of our overall algorithm are specified.</p>
  <p>So we designed the <span class="math inline">\(\text{FFT}\)</span>, a way to move from coefficients to values in time just <span class="math inline">\(O(n \log{n})\)</span>, when the points <span class="math inline">\(\{ x_i \}\)</span> are complex <span class="math inline">\(n\)</span>th roots of unity (<span class="math inline">\(1, \omega, \omega^2, \cdots, \omega^{n - 1}\)</span>).</p>
  <p><span class="math display">\[
  \langle \text{values} \rangle = \text{FFT}(\langle \text{coefficients} \rangle , \omega).
  \]</span></p>
  <p>The last remaining piece of the puzzle is the inverse operation, interpolation. It will turn out, amazingly, that</p>
  <p><span class="math display">\[
  \langle \text{coefficients} \rangle = \frac{1}{n} \text{FFT}(\langle \text{values} \rangle , \omega^{-1}).
  \]</span></p>
  <p>Interpolation is thus solved in the most simple and elegant way we could possibly have hoped for—using the same <span class="math inline">\(\text{FFT}\)</span> algorithm, but called with <span class="math inline">\(\omega^{-1}\)</span> in place of <span class="math inline">\(\omega\)</span>! This might seem like a miraculous coincidence, but it will make a lot more sense when we recast our polynomial operations in the language of linear algebra. Meanwhile, our <span class="math inline">\(O(n \log{n})\)</span> polynomial multiplication algorithm (Figure 2.5) is now fully specified.</p>
  <p><strong>A Matrix Reformulation</strong></p>
  <p>To get a clearer view of interpolation, let’s explicitly set down the relationship between our two representations for a polynomial <span class="math inline">\(A(x)\)</span> of degree <span class="math inline">\(\leq n - 1\)</span>. They are both vectors of <span class="math inline">\(n\)</span> numbers, and one is a linear transformation of the other:</p>
  <p><span class="math display">\[
  \begin{bmatrix} A(x_0) \\ A(x_1) \\ \vdots \\ A(x_{n - 1}) \end{bmatrix}
  =
  \begin{bmatrix}
  1      &amp; x_0     &amp;  x_0^2      &amp; \cdots &amp; x_0^{n - 1} \\
  1      &amp; x_1     &amp;  x_1^2      &amp; \cdots &amp; x_1^{n - 1} \\
  \vdots &amp; \vdots  &amp; \vdots      &amp; \cdots &amp; \vdots      \\
  1      &amp; x_{n-1} &amp; x_{n - 1}^2 &amp; \cdots &amp; x_{n - 1}^{n - 1}
  \end{bmatrix}
  \begin{bmatrix} a_0 \\ a_1 \\ \vdots \\ a_{n - 1} \end{bmatrix}
  \]</span></p>
  <p>Call the matrix in the middle <span class="math inline">\(M\)</span>. Its specialized format—a <em>Vandermonde matrix</em>—gives it many remarkable properties, of which the following is particularly relevant to us.</p>
  <ul>
  <li>If <span class="math inline">\(x_0, \cdots, x_{n - 1}\)</span> are distinct numbers, then <span class="math inline">\(M\)</span> is invertible.</li>
  </ul>
  <p>The existence of <span class="math inline">\(M^{-1}\)</span> allows us to invert the preceding matrix equation so as to express coefficients in terms of values. In brief,</p>
  <ul>
  <li>Evaluation is multiplication by <span class="math inline">\(M\)</span>, while interpolation is multiplication by <span class="math inline">\(M^{-1}\)</span>.</li>
  </ul>
  <p>This reformulation of our polynomial operations reveals their essential nature more clearly. Among other things, it finally justifies an assumption we have been making throughout, that <span class="math inline">\(A(x)\)</span> is uniquely characterized by its values at any <span class="math inline">\(n\)</span> points—in fact, we now have an explicit formula that will give us the coefficients of <span class="math inline">\(A(x)\)</span> in this situation.</p>
  <p>Vandermonde matrices also have the distinction of being quicker to invert than more general matrices, in <span class="math inline">\(O(n^2)\)</span> time instead of <span class="math inline">\(O(n^3)\)</span>. However, using this for interpolation would still not be fast enough for us, so once again we turn to our special choice of points—the complex roots of unity.</p>
  <p><strong>Interpolation Resolved</strong></p>
  <p>In linear algebra terms, the <span class="math inline">\(\text{FFT}\)</span> multiplies an arbitrary <span class="math inline">\(n\)</span>-dimensional vector—which we have been calling the <em>coefficient representation</em>—by the <span class="math inline">\(n \times n\)</span> matrix</p>
  <p><span class="math display">\[
  M_n(\omega) =
  \begin{bmatrix}
  1      &amp; 1     &amp;  1      &amp; \cdots &amp; 1 \\
  1      &amp; \omega     &amp;  \omega^2      &amp; \cdots &amp; \omega^{n - 1} \\
  1 &amp; \omega^2 &amp; \omega^4 &amp; \cdots &amp; \omega^{2(n - 1)} \\
  \vdots &amp; \vdots  &amp; \vdots      &amp; \cdots &amp; \vdots      \\
  1 &amp; \omega^{j} &amp; \omega^{2j} &amp; \cdots &amp; \omega^{j(n - 1)} \\
  \vdots &amp; \vdots  &amp; \vdots      &amp; \cdots &amp; \vdots      \\
  1 &amp; \omega^{(n - 1)} &amp; \omega^{2(n - 1)} &amp; \cdots &amp; \omega^{(n - 1)(n - 1)}
  \end{bmatrix}
  \begin{matrix}
  \longleftarrow &amp; \text{row for}\ \omega^0 = 1 \\ \longleftarrow &amp; \omega \\ \longleftarrow  &amp; \omega^2 \\ &amp; \vdots \\ \longleftarrow &amp; \omega^{j} \\ &amp; \vdots \\ \longleftarrow  &amp; \omega^{n - 1}
  \end{matrix}
  \]</span></p>
  <p>where <span class="math inline">\(\omega\)</span> is a complex <span class="math inline">\(n\)</span>th root of unity, and <span class="math inline">\(n\)</span> is a power of <span class="math inline">\(2\)</span>. Notice how simple this matrix is to describe: its (<span class="math inline">\(j, k\)</span>)th entry (starting row- and column-count at zero) is <span class="math inline">\(\omega^{jk}\)</span>.</p>
  <p>Multiplication by <span class="math inline">\(M = M_n(\omega)\)</span> maps the <span class="math inline">\(k\)</span>th coordinate axis (the vector with all zeros except for a <span class="math inline">\(1\)</span> at position <span class="math inline">\(k\)</span>) onto the <span class="math inline">\(k\)</span>th column of <span class="math inline">\(M\)</span>. Now here’s the crucial observation, which we’ll prove shortly: <em>the columns of <span class="math inline">\(M\)</span> are orthogonal (at right angles) to each other</em>. Therefore they can be thought of as the axes of an alternative coordinate system, which is often called the <em>Fourier basis</em>.</p>
  <p> </p>
  <center><div class="figure">
  <img src="fig-2.8-fourier-basis.png" alt="Figure 2.8 The \text{FFT} takes points in the standard coordinate system, whose axes are shown here as x_1, x_2, x_3, and rotates them into the Fourier basis, whose axes are the columns of M_n(\omega), shown here as f_1, f_2, f_3. For instance, points in direction x_1 get mapped into direction f_1." />
  <p class="caption"><strong>Figure 2.8</strong> The <span class="math inline">\(\text{FFT}\)</span> takes points in the standard coordinate system, whose axes are shown here as <span class="math inline">\(x_1, x_2, x_3\)</span>, and rotates them into the Fourier basis, whose axes are the columns of <span class="math inline">\(M_n(\omega)\)</span>, shown here as <span class="math inline">\(f_1, f_2, f_3\)</span>. For instance, points in direction <span class="math inline">\(x_1\)</span> get mapped into direction <span class="math inline">\(f_1\)</span>.</p>
</div></center>
  <p> </p>
  <p>The effect of multiplying a vector by <span class="math inline">\(M\)</span> is to rotate it from the standard basis, with the usual set of axes, into the Fourier basis, which is defined by the columns of <span class="math inline">\(M\)</span> (Figure 2.8). The FFT is thus a change of basis, a <em>rigid rotation</em>. The inverse of <span class="math inline">\(M\)</span> is the opposite rotation, from the Fourier basis back into the standard basis. When we write out the orthogonality condition precisely, we will be able to read off this inverse transformation with ease:</p>
  <p><span class="math inline">\(\textbf{Inversion formula}\ \)</span> <span class="math inline">\(M_n(\omega)^{-1} = \frac{1}{n}M_n(\omega^{-1})\)</span>.</p>  <p>But <span class="math inline">\(\omega^{-1}\)</span> is also an <span class="math inline">\(n\)</span>th root of unity, and so interpolation—or equivalently, multiplication by <span class="math inline">\(M_n(\omega)^{-1}\)</span>—is itself just an <span class="math inline">\(\text{FFT}\)</span> operation, but with <span class="math inline">\(\omega\)</span> replaced by <span class="math inline">\(\omega^{-1}\)</span>.</p>
  <p>Now let's get into the details. Take <span class="math inline">\(\omega\)</span> to be <span class="math inline">\(e^{2 \pi i / n}\)</span> for convenience, and think of the columns of <span class="math inline">\(M\)</span> as vectors in <span class="math inline">\(\mathbb{C}^{n}\)</span>. Recall that the <em>angle</em> between two vectors <span class="math inline">\(u = (u_0, \cdots, u_{n - 1})\)</span> and <span class="math inline">\(v = (v_0, \cdots, v_{n - 1})\)</span> in <span class="math inline">\(\mathbb{C}^{n}\)</span> is just a scaling factor times their <em>inner product</em></p>
  <p><span class="math display">\[
  u \cdot v^{\ast} = u_0 v_0^{\ast} + u_1 v_1^{\ast} + \cdots + u_{n - 1} v_{n - 1}^{\ast},
  \]</span></p>
  <p>where <span class="math inline">\(z^{\ast}\)</span> denotes the complex conjugate<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> of <span class="math inline">\(z\)</span>. This quantity is maximized when the vectors lie in the same direction and is zero when the vectors are orthogonal to each other.</p>
  <p>The fundamental observation we need is the following.</p>
  <p><span class="math inline">\(\textbf{Lemma}\ \)</span> The columns of matrix <span class="math inline">\(M\)</span> are orthogonal to each other.</p>
  <p><span class="math inline">\(\textit{Proof}.\ \)</span> Take the inner product of any columns <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> of matrix <span class="math inline">\(M\)</span>,</p>
  <p><span class="math display">\[
  1 + \omega^{j - k} + \omega^{2(j - k)} + \cdots + \omega^{(n - 1)(j - k)}.
  \]</span></p>
  <p>This is a geometric series with first term <span class="math inline">\(1\)</span>, last term <span class="math inline">\(\omega^{(n - 1)(j - k)}\)</span>, and ratio <span class="math inline">\(\omega^{(j - k)}\)</span>. Therefore it evaluates to <span class="math inline">\((1 - \omega^{n(j - k)}) / (1 - \omega^{(j - k)})\)</span>, which is <span class="math inline">\(0\)</span>—except when <span class="math inline">\(j = k\)</span>, in which case all terms are <span class="math inline">\(1\)</span> and the sum is <span class="math inline">\(n\)</span>. <span class="math inline">\(\blacksquare\)</span></p>
  <p>The orthogonality property can be summarized in the single equation</p>
  <p><span class="math display">\[
  MM^{\ast} = nI,
  \]</span></p>
  <p>since <span class="math inline">\({(MM^{\ast})}_{ij}\)</span> is the inner product of the <span class="math inline">\(i\)</span>th and <span class="math inline">\(j\)</span>th columns of <span class="math inline">\(M\)</span> (do you see why?). This immediately implies <span class="math inline">\(M^{-1} = \frac{1}{n} M^{\ast}\)</span>: we have an inversion formula! But is it the same formula we earlier claimed?</p>
  <p>Let’s see—the (<span class="math inline">\(j, k\)</span>)th entry of <span class="math inline">\(M^{\ast}\)</span> is the complex conjugate of the corresponding entry of <span class="math inline">\(M\)</span>, in other words <span class="math inline">\(\omega^{-jk}\)</span>. Whereupon <span class="math inline">\(M^{\ast} = M_n(\omega^{-1})\)</span>, and we’re done.</p>
  <p>And now we can finally step back and view the whole affair geometrically. The task we need to perform, polynomial multiplication, is a lot easier in the Fourier basis than in the standard basis. Therefore, we first rotate vectors into the Fourier basis (<em>evaluation</em>), then perform the task (<em>multiplication</em>), and finally rotate back (<em>interpolation</em>). The initial vectors are <em>coefficient representations</em>, while their rotated counterparts are <em>value representations</em>. To efficiently switch between these, back and forth, is the province of the <span class="math inline">\(\text{FFT}\)</span>.</p>
  <p> </p>
  <h3 id="a-closer-look-at-the-fast-fourier-transform">2.6.4 A Closer Look at the Fast Fourier Transform</h3>
  <p>Now that our efficient scheme for polynomial multiplication is fully realized, let’s hone in more closely on the core subroutine that makes it all possible, the fast Fourier transform.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> FFT(a, w):
    <span class="co">&quot;&quot;&quot;</span>
  <span class="co">  Input: an array a_0, a_1, ..., a_(n - 1), for n a power of 2</span>
  <span class="co">         a primitive nth root of unity, w</span>
  <span class="co">  Output: Mn(w)a</span>
  <span class="co">  &quot;&quot;&quot;</span>
    <span class="cf">if</span> w <span class="op">==</span> <span class="dv">1</span>:
      <span class="cf">return</span> a
    <span class="cf">else</span>:
      s_0, s_1, ..., s_{n <span class="op">/</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>} <span class="op">=</span> FFT((a_0, a_2, ..., a_{n <span class="op">-</span> <span class="dv">2</span>}), w<span class="op">^</span><span class="dv">2</span>)
      s<span class="op">^</span><span class="dv">0</span>, s<span class="op">^</span><span class="dv">1</span>, ..., s<span class="op">^</span>{n <span class="op">/</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>} <span class="op">=</span> FFT((a_1, a_3, ..., a_{n <span class="op">-</span> <span class="dv">1</span>}), w<span class="op">^</span><span class="dv">2</span>)
      <span class="cf">for</span> i <span class="op">=</span> <span class="dv">0</span> to n <span class="op">/</span> <span class="dv">2</span> <span class="op">-</span> <span class="dv">1</span>:
        r_j <span class="op">=</span> s_0 <span class="op">+</span> w<span class="op">^</span>j <span class="op">*</span> s<span class="op">^</span><span class="dv">0</span>
        r_{j <span class="op">+</span> n <span class="op">/</span> <span class="dv">2</span>} <span class="op">=</span> s_0 <span class="op">-</span> w<span class="op">^</span>j <span class="op">*</span> s<span class="op">^</span><span class="dv">0</span>
      <span class="cf">return</span> (r_0, r_1, ..., r_{n <span class="op">-</span> <span class="dv">1</span>})</code></pre></div>
  <p><strong>The definitive FFT algorithm</strong> <p>
  <p> The <span class="math inline">\(\text{FFT}\)</span> takes as input a vector <span class="math inline">\(a = (a_0, \cdots, a_{n - 1})\)</span> and a complex number <span class="math inline">\(\omega\)</span> whose powers <span class="math inline">\(1, \omega, \omega^2, \cdots, \omega^{n - 1}\)</span> are the complex <span class="math inline">\(n\)</span>th roots of unity. It multiplies vector <span class="math inline">\(a\)</span> by the <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(M_{n}(\omega)\)</span>, which has (<span class="math inline">\(j, k\)</span>)th entry (starting row- and column-count at zero) <span class="math inline">\(\omega^{jk}\)</span>. The potential for using divide-and-conquer in this matrix-vector multiplication becomes apparent when <span class="math inline">\(M\)</span>'s columns are segregated into evens and odds:</p>
  <div class="figure">
  <img src="M-matrix.png" />

  </div>
  <p>In the second step, we have simplified entries in the bottom half of the matrix using <span class="math inline">\(\omega^{n / 2} = -1\)</span> and <span class="math inline">\(\omega^{n} = 1\)</span>. Notice that the top left <span class="math inline">\(n / 2 \times n / 2\)</span> submatrix is <span class="math inline">\(M_{n/2}(\omega^2)\)</span>, as is the one on the bottom left. And the top and bottom right submatrices are almost the same as <span class="math inline">\(M_{n/2}(\omega^2)\)</span>, but with their <span class="math inline">\(j\)</span>th rows multiplied through by <span class="math inline">\(\omega^{j}\)</span> and <span class="math inline">\(-\omega^{j}\)</span>, respectively. Therefore the final product is the vector</p>
  <div class="figure">
  <img src="divide-and-conquer-M.png" />

  </div>
  <p>In short, the product of <span class="math inline">\(M_{n}(\omega)\)</span> with vector <span class="math inline">\(a = (a_0, \cdots, a_{n - 1})\)</span>, a size-<span class="math inline">\(n\)</span> problem, can be expressed in terms of two size-<span class="math inline">\(n / 2\)</span> problems: the product of <span class="math inline">\(M_{n / 2}(\omega^2)\)</span> with <span class="math inline">\((a_0, a_2, \cdots, a_{n - 2})\)</span> and with <span class="math inline">\((a_1, a_3, \cdots, a_{n - 1})\)</span>. This divide-and-conquer strategy leads to the definitive FFT algorithm of Figure 2.9, whose running time is <span class="math inline">\(T(n) = 2T(n / 2) + O(n) = O(n \log{n})\)</span>.</p>
  <p><strong>The fast Fourier transform unraveled</strong></p>
  <p>Throughout all our discussions so far, the fast Fourier transform has remained tightly co- cooned within a divide-and-conquer formalism. To fully expose its structure, we now unravel the recursion.</p>
  <p>The divide-and-conquer step of the FFT can be drawn as a very simple circuit. Here is how a problem of size <span class="math inline">\(n\)</span> is reduced to two subproblems of size <span class="math inline">\(n / 2\)</span> (for clarity, one pair of outputs (<span class="math inline">\(j, j + n / 2\)</span>) is singled out):</p>
  <div class="figure">
  <img src="FFT-circuit.png" />

  </div>
  <p>We’re using a particular shorthand: the edges are wires carrying complex numbers from left to right. A weight of <span class="math inline">\(j\)</span> means &quot;multiply the number on this wire by <span class="math inline">\(\omega^{j}\)</span>.&quot; And when two wires come into a junction from the left, the numbers they are carrying get added up. So the two outputs depicted are executing the commands</p>
  <p><span class="math display">\[
  r_j = s_j + \omega^{j}s_j&#39;
  \]</span> <span class="math display">\[
  r_{j + n / 2} = s_j - \omega^{j}s_j&#39;
  \]</span></p>
  <p>from the <span class="math inline">\(\text{FFT}\)</span> algorithm (Figure 2.9), via a pattern of wires known as a <a href="https://en.wikipedia.org/wiki/Butterfly_diagram"><em>butterfly</em></a>.</p>
  <p>Unraveling the FFT circuit completely for <span class="math inline">\(n = 8\)</span> elements, we get Figure 10.4. Notice the following.</p>
  <ol style="list-style-type: decimal">
  <li><p>For <span class="math inline">\(n\)</span> inputs there are <span class="math inline">\(\log_{2}{n}\)</span> levels, each with <span class="math inline">\(n\)</span> nodes, for a total of <span class="math inline">\(n \log{n}\)</span> operations.</p></li>
  <li><p>The inputs are arranged in a peculiar order: <span class="math inline">\(0, 4, 2, 6, 1, 5, 3, 7\)</span>.</p></li>
  </ol>
  <p>Why? Recall that at the top level of recursion, we first bring up the even coefficients of the input and then move on to the odd ones. Then at the next level, the even coefficients of this first group (which therefore are multiples of <span class="math inline">\(4\)</span>, or equivalently, have zero as their two least significant bits) are brought up, and so on.</p>
  <p>To put it otherwise, the inputs are arranged by increasing last bit of the binary representation of their index, resolving ties by looking at the next more significant bit(s). The resulting order in binary, <span class="math inline">\(000,100,010,110,001,101,011,111\)</span>, is the same as the natural one, <span class="math inline">\(000, 001, 010, 011, 100, 101, 110, 111\)</span> except the bits are mirrored!</p>
  <ol start="3" style="list-style-type: decimal">
  <li>There is a unique path between each input <span class="math inline">\(a_j\)</span> and each output <span class="math inline">\(A(\omega^{k})\)</span>.</li>
  </ol>
  <p>This path is most easily described using the binary representations of <span class="math inline">\(j\)</span> and <span class="math inline">\(k\)</span> (shown in Figure 10.4 for convenience). There are two edges out of each node, one going up (the <span class="math inline">\(0\)</span>-edge) and one going down (the <span class="math inline">\(1\)</span>-edge). To get to <span class="math inline">\(A(\omega^{k})\)</span> from any input node, simply follow the edges specified in the bit representation of <span class="math inline">\(k\)</span>, starting from the rightmost bit. (Can you similarly specify the path in the reverse direction?)</p>
  <ol start="4" style="list-style-type: decimal">
  <li>On the path between <span class="math inline">\(a_j\)</span> and <span class="math inline">\(A(\omega^{k})\)</span>, the labels add up to <span class="math inline">\(jk \bmod{8}\)</span>.</li>
  </ol>
  <p>Since <span class="math inline">\(\omega^8 = 1\)</span>, this means that the contribution of input <span class="math inline">\(a_j\)</span> to output <span class="math inline">\(A(\omega^{k})\)</span> is <span class="math inline">\(a_j \omega^{jk}\)</span>, and therefore the circuit computes correctly the values of polynomial <span class="math inline">\(A(x)\)</span>.</p>
  <ol start="5" style="list-style-type: decimal">
  <li>And finally, notice that the FFT circuit is a natural for parallel computation and direct implementation in hardware.</li>
  </ol>
  <p> </p>
  <center><div class="figure">
  <img src="fig-2.10-FFT-circuit.png" alt="Figure 2.10 The fast Fourier transform circuit." />
  <p class="caption"><strong>Figure 2.10</strong> The fast Fourier transform circuit.</p>
  </div></center>
  <p> </p>
  <blockquote>
  <p><strong>The Spread of a Fast Algorithm</strong></p>
  <p>In 1963, during a meeting of President Kennedy’s scientific advisors, John Tukey, a mathematician from Princeton, explained to IBM’s Dick Garwin a fast method for computing Fourier transforms. Garwin listened carefully, because he was at the time working on ways to detect nuclear explosions from seismographic data, and Fourier transforms were the bottleneck of his method. When he went back to IBM, he asked John Cooley to implement Tukey’s algorithm; they decided that a paper should be published so that the idea could not be patented.</p>
  <p>Tukey was not very keen to write a paper on the subject, so Cooley took the initiative. And this is how one of the most famous and most cited scientific papers was published in 1965, co-authored by Cooley and Tukey. The reason Tukey was reluctant to publish the FFT was not secretiveness or pursuit of profit via patents. He just felt that this was a simple observation that was probably already known. This was typical of the period: back then (and for some time later) algorithms were considered second-class mathematical objects, devoid of depth and elegance, and unworthy of serious attention.</p>
  <p>But Tukey was right about one thing: it was later discovered that British engineers had used the FFT for hand calculations during the late 1930s. And—to end this chapter with the same great mathematician who started it—a paper by Gauss in the early 1800s on (what else?) interpolation contained essentially the same idea in it! Gauss’s paper had remained a secret for so long because it was protected by an old-fashioned cryptographic technique: like most scientific papers of its era, it was written in Latin.</p>
  </blockquote>
  <p> </p>
  <div class="footnotes">
  <hr />
  <ol>
  <li id="fn1"><p>In a typical setting for polynomial multiplication, the coefficients of the polynomials are real numbers and, moreover, are small enough that basic arithmetic operations (adding and multiplying) take unit time. We will assume this to be the case without any great loss of generality; in particular, the time bounds we obtain are easily adjustable to situations with larger numbers.<a href="#fnref1">↩</a></p></li>
  <li id="fn2"><p>The complex conjugate of a complex number <span class="math inline">\(z = r e^{i \theta}\)</span> is <span class="math inline">\(z^{\ast} = r e^{-i \theta}\)</span>. The complex conjugate of a vector (or matrix) is obtained by taking the complex conjugates of all its entries.<a href="#fnref2">↩</a></p></li>
  </ol>
  </div>

  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
