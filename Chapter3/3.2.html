<!DOCTYPE html>
<html>
<head>

  <title>3.2 Depth-First Search</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="depth-first-search-in-undirected-graphs">3.2 Depth-First Search in Undirected Graphs</h2>
  <h3 id="exploring-mazes">3.2.1 Exploring Mazes</h3>
  <p><strong>Depth-first search</strong> is a surprisingly versatile linear-time procedure that reveals a wealth of information about a graph. The most basic question it addresses is,</p>
  <ul>
  <li>What parts of the graph are reachable from a given vertex?</li>
  </ul>
  <p>To understand this task, try putting yourself in the position of a computer that has just been given a new graph, say in the form of an adjacency list. This representation offers just one basic operation: finding the neighbors of a vertex. With only this primitive, the reachability problem is rather like exploring a labyrinth (Figure 3.2).</p>
  <div class="figure">
  <img src="fig-3.2-graph-maze-exploration.png" alt="Figure 3.2 Exploring a graph is rather like navigating a maze." />
  <p class="caption"><strong>Figure 3.2</strong> Exploring a graph is rather like navigating a maze.</p>
  </div>
  <p> </p>
  <p>You start walking from a fixed place and whenever you arrive at any junction (vertex) there are a variety of passages (edges) you can follow. A careless choice of passages might lead you around in circles or might cause you to overlook some accessible part of the maze. Clearly, you need to record some intermediate information during exploration.</p>
  <p>This classic challenge has amused people for centuries. Everybody knows that all you need to explore a labyrinth is a ball of string and a piece of chalk. The chalk prevents looping, by marking the junctions you have already visited. The string always takes you back to the starting place, enabling you to return to passages that you previously saw but did not yet investigate.</p>
  <p>How can we simulate these two primitives, chalk and string, on a computer? The chalk marks are easy: for each vertex, maintain a Boolean variable indicating whether it has been visited already. As for the ball of string, the correct cyberanalog is a <em>stack</em>. After all, the exact role of the string is to offer two primitive operations—<em>unwind</em> to get to a new junction (the stack equivalent is to <em>push</em> the new vertex) and <em>rewind</em> to return to the previous junction (<em>pop</em> the stack).</p>
  <p>Instead of explicitly maintaining a stack, we will do so implicitly via recursion (which is implemented using a stack of activation records). The resulting algorithm is shown in Figure 3.3.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> The <span class="math inline">\(\texttt{previsit}\)</span> and <span class="math inline">\(\texttt{postvisit}\)</span> procedures are optional, meant for performing operations on a vertex when it is first discovered and also when it is being left for the last time. We will soon see some creative uses for them.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> explore(G, v):
    <span class="co">&quot;&quot;&quot;</span>
  <span class="co">  Input: G = (V,E) is a graph; v ∈ V</span>
  <span class="co">  Output: visited(u) is set to true for all nodes u reachable from v</span>
  <span class="co">  &quot;&quot;&quot;</span>

    visited(v) <span class="op">=</span> true
    previsit(v)

    <span class="cf">for</span> each edge (v, u) ∈ E:
      <span class="cf">if</span> <span class="kw">not</span> visited(u): explore(u)
    postvisit(v)

    <span class="cf">return</span> visited</code></pre></div>
  <p>More immediately, we need to confirm that explore always works correctly. It certainly does not venture too far, because it only moves from nodes to their neighbors and can therefore never jump to a region that is not reachable from v. But does it find <em>all</em> vertices reachable from <span class="math inline">\(v\)</span>? Well, if there is some <span class="math inline">\(u\)</span> that it misses, choose any path from <span class="math inline">\(v\)</span> to <span class="math inline">\(u\)</span>, and look at the last vertex on that path that the procedure actually visited. Call this node <span class="math inline">\(z\)</span>, and let <span class="math inline">\(w\)</span> be the node immediately after it on the same path.</p>
  <div class="figure">
  <img src="v-to-u-path.png" />

  </div>
  <p>So <span class="math inline">\(z\)</span> was visited but <span class="math inline">\(w\)</span> was not. This is a contradiction: while the <span class="math inline">\(\texttt{explore}\)</span> procedure was at node <span class="math inline">\(z\)</span>, it would have noticed <span class="math inline">\(w\)</span> and moved on to it.</p>
  <p>Incidentally, this pattern of reasoning arises often in the study of graphs and is in essence a streamlined induction. A more formal inductive proof would start by framing a hypothesis, such as &quot;for any <span class="math inline">\(k \geq 0\)</span>, all nodes within <span class="math inline">\(k\)</span> hops from <span class="math inline">\(v\)</span> get visited.&quot; The base case is as usual trivial, since <span class="math inline">\(v\)</span> is certainly visited. And the general case—showing that if all nodes <span class="math inline">\(k\)</span> hops away are visited, then so are all nodes <span class="math inline">\(k + 1\)</span> hops away—is precisely the same point we just argued.</p>
  <p>Figure 3.4 shows the result of running <span class="math inline">\(\texttt{explore}\)</span> on our earlier example graph, starting at node <span class="math inline">\(A\)</span>, and breaking ties in alphabetical order whenever there is a choice of nodes to visit. The solid edges are those that were actually traversed, each of which was elicited by a call to explore and led to the discovery of a new vertex. For instance, while <span class="math inline">\(B\)</span> was being visited, the edge <span class="math inline">\(B − E\)</span> was noticed and, since <span class="math inline">\(E\)</span> was as yet unknown, was traversed via a call to <span class="math inline">\(\texttt{explore}(E)\)</span>.</p>
  <div class="figure">
  <img src="fig-3.4-explore-example.png" alt="Figure 3.4 The result of \texttt{explore}(A) on the graph of Figure 3.2." />
  <p class="caption"><strong>Figure 3.4</strong> The result of <span class="math inline">\(\texttt{explore}(A)\)</span> on the graph of Figure 3.2.</p>
  </div>
  <p> </p>
  <p>These solid edges form a tree (a connected graph with no cycles) and are therefore called <strong>tree edges</strong>. The dotted edges were ignored because they led back to familiar terrain, to vertices previously visited. They are called <strong>back edges</strong>.</p>
  <p> </p>
  <h3 id="depth-first-search">3.2.2 Depth-First Search</h3>
  <p>The <span class="math inline">\(\texttt{explore}\)</span> procedure visits only the portion of the graph reachable from its starting point. To examine the rest of the graph, we need to restart the procedure elsewhere, at some vertex that has not yet been visited. The algorithm of Figure 3.5, called <strong>depth-first search</strong> (DFS), does this repeatedly until the entire graph has been traversed.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> dfs(G):

    <span class="cf">for</span> <span class="bu">all</span> v ∈ V:
      visited(v) <span class="op">=</span> false

    <span class="cf">for</span> <span class="bu">all</span> v ∈ V :
      <span class="cf">if</span> <span class="kw">not</span> visited(v): explore(v)</code></pre></div>
  <p>The first step in analyzing the running time of DFS is to observe that each vertex is <span class="math inline">\(\texttt{explore}\)</span>’d just once, thanks to the visited array (the chalk marks). During the exploration of a vertex, there are the following steps:</p>
  <ol style="list-style-type: decimal">
  <li><p>Some fixed amount of work—marking the spot as visited, and the <span class="math inline">\(\texttt{pre / postvisit}\)</span>.</p></li>
  <li><p>A loop in which adjacent edges are scanned, to see if they lead somewhere new.</p></li>
  </ol>
  <p>This loop takes a different amount of time for each vertex, so let’s consider all vertices together. The total work done in step 1 is then <span class="math inline">\(O(|V|)\)</span>. In step 2, over the course of the entire DFS, each edge <span class="math inline">\(\{x, y\} \in E\)</span> is examined exactly twice, once during <span class="math inline">\(\texttt{explore}(x)\)</span> and once during <span class="math inline">\(\texttt{explore}(y)\)</span>. The overall time for step 2 is therefore <span class="math inline">\(O(|E|)\)</span> and so the depth-first search has a running time of <span class="math inline">\(O(|V| + |E|)\)</span>, linear in the size of its input. This is as efficient as we could possibly hope for, since it takes this long even just to read the adjacency list.</p>
  <p>Figure 3.6 shows the outcome of depth-first search on a <span class="math inline">\(12\)</span>-node graph, once again breaking ties alphabetically (ignore the pairs of numbers for the time being). The outer loop of DFS calls explore three times, on <span class="math inline">\(A, C,\)</span> and finally <span class="math inline">\(F\)</span>. As a result, there are three trees, each rooted at one of these starting points. Together they constitute a <em>forest</em>.</p>
  <div class="figure">
  <img src="fig-3.6-dfs-example.png" alt="Figure 3.6 (a) A 12-node graph. (b) DFS search forest." />
  <p class="caption"><strong>Figure 3.6</strong> (a) A 12-node graph. (b) DFS search forest.</p>
  </div>
  <p> </p>
  <h3 id="connectivity-in-undirected-graphs">3.2.3 Connectivity in Undirected Graphs</h3>
  <p>An undirected graph is <em>connected</em> if there is a path between any pair of vertices. The graph of Figure 3.6 is <em>not</em> connected because, for instance, there is no path from <span class="math inline">\(A\)</span> to <span class="math inline">\(K\)</span>. However, it does have three disjoint connected regions, corresponding to the following sets of vertices:</p>
  <p><span class="math display">\[\begin{matrix} \{A, B, E, I, J\} &amp;&amp; \{C, D, G, H, K, L\} &amp;&amp; \{F\} \end{matrix}\]</span></p>
  <p>These regions are called <strong>connected components</strong>: each of them is a subgraph that is internally connected but has no edges to the remaining vertices. When <span class="math inline">\(\text{explore}\)</span> is started at a particular vertex, it identifies precisely the connected component containing that vertex. And each time the DFS outer loop calls <span class="math inline">\(\text{explore}\)</span>, a new connected component is picked out.</p>
  <p>Thus depth-first search is trivially adapted to check if a graph is connected and, more generally, to assign each node <span class="math inline">\(v\)</span> an integer <span class="math inline">\(\texttt{ccnum}[v]\)</span> identifying the connected component to which it belongs. All it takes is</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> previsit(v):
    ccnum[v] <span class="op">=</span> cc</code></pre></div>
  <p>where <span class="math inline">\(\texttt{cc}\)</span> needs to be initialized to zero and to be incremented each time the DFS procedure calls <span class="math inline">\(\texttt{explore}\)</span>.</p>
  <p> </p>
  <h3 id="previsit-and-postvisit-orderings">3.2.4 Previsit and Postvisit Orderings</h3>
  <p>We have seen how depth-first search—a few unassuming lines of code—is able to uncover the connectivity structure of an undirected graph in just linear time. But it is far more versatile than this. In order to stretch it further, we will collect a little more information during the exploration process: for each node, we will note down the times of two important events, the moment of first discovery (corresponding to <span class="math inline">\(\texttt{previsit}\)</span>) and that of final departure (<span class="math inline">\(\texttt{postvisit}\)</span>). Figure 3.6 shows these numbers for our earlier example, in which there are <span class="math inline">\(24\)</span> events. The fifth event is the discovery of <span class="math inline">\(I\)</span>. The <span class="math inline">\(21\)</span>st event consists of leaving <span class="math inline">\(D\)</span> behind for good.</p>
  <p>One way to generate arrays <span class="math inline">\(\texttt{pre}\)</span> and <span class="math inline">\(\texttt{post}\)</span> with these numbers is to define a simple counter <span class="math inline">\(\texttt{clock}\)</span>, initially set to <span class="math inline">\(1\)</span>, which gets updated as follows.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">
  <span class="kw">def</span> previsit(v):
    pre[v] <span class="op">=</span> clock
    clock  <span class="op">=</span> clock <span class="op">+</span> <span class="dv">1</span>

  <span class="kw">def</span> postvisit(v):
    post[v]<span class="op">=</span> clock
    clock  <span class="op">=</span> clock <span class="op">+</span> <span class="dv">1</span></code></pre></div>
  <p>These timings will soon take on larger significance. Meanwhile, you might have noticed from Figure 3.4 that:</p>
  <p><strong>Property</strong> <em>For any nodes <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span>, the two intervals <span class="math inline">\([\texttt{pre}(u), \texttt{post}(u)]\)</span> and <span class="math inline">\([\texttt{pre}(v), \texttt{post}(v)]\)</span> are either disjoint or one is contained within the other.</em></p>
  <p>Why? Because <span class="math inline">\([\texttt{pre}(u), \texttt{post}(u)]\)</span> is essentially the time during which vertex <span class="math inline">\(u\)</span> was on the stack. The last-in, first-out behavior of a stack explains the rest.</p>
  <p> </p>
  <div class="footnotes">
  <hr />
  <ol>
  <li id="fn1"><p>As with many of our graph algorithms, this one applies to both undirected and directed graphs. In such cases, we adopt the <em>directed notation</em> for edges, <span class="math inline">\((x, y)\)</span>. If the graph is undirected, then each of its edges should be thought of as existing in both directions: <span class="math inline">\((x, y)\)</span> and <span class="math inline">\((y, x)\)</span>.<a href="#fnref1">↩</a></p></li>
  </ol>
  </div>

  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
