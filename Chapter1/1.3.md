## 1.3 Primality Testing

Is there some litmus test that will tell us whether a number is prime without actually trying to factor the number? We place our hopes in a theorem from the year 1640.

### Fermat's Little Theorem
If $p$ is prime, then for every $1 \leq a < p$,

$$
a^{p-1} \equiv 1 \bmod{p}.
$$

*Proof.* Let $S$ be the nonzero integers modulo $p$; that is, $S = \{ 1, 2, \cdots, p - 1 \}$. Here's the crucial observation: the effect of multiplying these numbers by $a \bmod{p}$ is simply to permute them. For instance, here's a picture of the case $a = 3, p = 7$:

![](fermat-little-theorem.png)

Let's carry this example a bit further. From the picture, we can conclude

$$
\{1, 2, \cdots, 6\} = \{3 \cdot 1  \bmod{7}, 3 \cdot 2 \bmod{7}, \cdots, 3 \cdot 6 \bmod{7} \}.
$$

Multiplying all the numbers in each representation then gives $6! \equiv 36 \cdot 6! \bmod{7}$, and dividing by $6!$ we get $3^6 \equiv 1 \bmod{7}$, exactly the result we wanted in the case $a = 3, p = 7$.

Now let's generalize this argument to other values of $a$ and $p$, with $S = \{1, 2, \cdots, p - 1\}$. We'll prove that when the elements of $S$ are multiplied by $a \bmod{p}$, the resulting numbers are all distinct and nonzero. And since they lie in the range $[1, p - 1]$, they must simply be a permutation of $S$.

The numbers $a \cdot i \bmod{p}$ are distinct because if $a \cdot i \equiv a \cdot j \bmod{p}$, then dividing both sides by $a$ gives $i \equiv j \bmod{p}$. They are nonzero because $a \cdot i \equiv 0$ similarly implies $i \equiv 0$. (And we *can* divide by $a$, because by assumption it is nonzero and therefore relatively prime to $p$.)

We now have two ways to write set $S$:

$$
S = \{1, 2, \cdots, p - 1\} = \{a \cdot 1 \bmod{p}, a \cdot 2 \bmod{p}, \cdots, a \cdot (p - 1) \bmod{p}\}.
$$

We multiply together its elements in each of these representations to get

$$
(p - 1)! \equiv a^{p - 1} \cdot (p - 1)! \bmod{p}.
$$

Dividing by $(p - 1)!$ (which we can do because it is relatively prime to $p$, since $p$ is assumed to be prime) then gives the theorem. $\blacksquare$


This theorem suggests a "factorless" test for determining whether a number N is prime:

![](fermat-test.png)

```python
def primality(N):
    """
    Input: positive integer N
    Output: yes (true), no (false)
    """
    a = random positive integer < N
    if a^(N - 1) == 1 (mod N):
        return True
    else:
        return False
```

The problem is that Fermat's theorem is not an if-and-only-if condition; it doesn't say what happens when $N$ is not prime, so in these cases the preceding diagram is questionable. In fact, it is possible for a composite number $N$ to pass Fermat's test (that is, $a^{N - 1} \equiv 1 \bmod{N}$) for certain choices of $a$.

For instance, $341 = 11 \cdot 31$ is not prime, and yet $2^{340} \equiv 1 \bmod{341}$. Nonetheless, we might hope that for composite $N$, most values of $a$ will fail the test. This is indeed true, in a sense we will shortly make precise, and motivates the algorithm of Figure 1.7: rather than fixing an arbitrary value of $a$ in advance, we should choose it *randomly* from $\{1, \cdots, N - 1\}$.

In analyzing the behavior of this algorithm, we first need to get a minor bad case out of the way. It turns out that certain extremely rare composite numbers $N$, called *Carmichael numbers*, pass Fermat's test for all $a$ relatively prime to $N$. On such numbers our algorithm will fail; but they are pathologically rare, and we will later see how to deal with them (page 38), so let's ignore these numbers for the time being.

In a Carmichael-free universe, our algorithm works well. Any prime number $N$ will of course pass Fermat's test and produce the right answer. On the other hand, any non-Carmichael composite number $N$ must fail Fermat's test for some value of $a$; and as we will now show, this implies immediately that $N$ fails Fermat's test for *at least half the possible values of* $a$!

**Lemma**
If $a^{N - 1} \not\equiv 1 \bmod{N}$ for some $a$ relatively prime to $N$, then it must hold for at least half the choices of $a < N$.

*Proof.* Fix some value of $a$ for which $a^{N - 1} \not\equiv 1 \bmod{N}$. The key is to notice that every element $b < N$ that passes Fermat's tests with respect to $N$ (that is, $b^{N - 1} \equiv 1 \bmod{N}$) has a twin, $a \cdot b$, that fails the test:

$$
(a \cdot b)^{N - 1} \equiv a^{N - 1} \cdot b^{N - 1} \equiv a^{N - 1} \not\equiv 1 \bmod{N}.
$$

Moreover, all these elements $a \cdot b$, for fixed $a$ but different choices of $b$, are distinct, for the
same reason $a \cdot i \not\equiv a \cdot j$ in the proof of Fermat's test: just divide by $a$.

![](fermat-test-lemma.png)

The one-to-one function $b \rightarrow a \cdot b$ shows that at least as many elements fail the test as pass it.



> **Hey, that was group theory!**
>
> For any integer $N$, the set of all numbers modulo $N$ that are relatively prime to $N$ constitute what mathematicians call a *group*:
>
>  * There is a multiplication operation defined on this set.
>  * The set contains a neutral element (namely 1: any number multiplied by this remains unchanged).
>  * All elements have a well-defined inverse.
  This particular group is called the *multiplicative group of* $N$, usually denoted $\mathbb{Z}^{\times}_N$.
> * Group theory is a very well developed branch of mathematics. One of its key concepts is that a group can contain a $subgroup$ â€“ a subset that is a group in and of itself. And an important fact about a subgroup is that its size must divide the size of the whole group.
>
> Consider now the set $B = \{b:b^{N - 1} \equiv 1 \bmod{N}\}$. It is not hard to see that it is a subgroup of $\mathbb{Z}^{\times}_N$ (just check that $B$ is closed under multiplication and inverses).
>
> Thus the size of $B$ must divide that of $\mathbb{Z}^{\times}_N$. Which means that if $B$ doesn't contain all of $\mathbb{Z}^{\times}_N$, the next largest size it can have is $|\mathbb{Z}^{\times}_N| / 2$.


We are ignoring Carmichael numbers, so we can now assert

+ If $N$ is prime, then $a^{N - 1} \equiv 1 \bmod{N}$ for all $a < N$.

+ If $N$ is not prime, then $a^{N - 1} \equiv 1 \mod[N]$ for at most half the values of $a < N$.

The algorithm of Figure 1.7 therefore has the following probabilistic behavior.

+ $\mathbb{P}(\text{Algorithm returns yes when input is prime}) = 1$

+ $\mathbb{P}(\text{Algorithm returns yes when input is not prime}) \leq \frac{1}{2}$


```python
def primality_fixed(N):
    """
    Input: positive integer N
    Output: yes (true), no (false)
    """
    a_1, ..., a_k = random positive integers < N
    if a_i == 1 (mod N) for all i = 1, ..., k:
        return True
    else:
        return False
```

We can reduce this *one-sided error* by repeating the procedure many times, by randomly picking several values of a and testing them all (Figure 1.8).

$$
\mathbb{P}(\text{Algorithm returns yes when input is not prime}) \leq \frac{1}{2^k}
$$

This probability of error drops exponentially fast, and can be driven arbitrarily low by choosing $k$ large enough. Testing $k = 100$ values of $a$ makes the probability of failure at most $2^{-100}$, which is miniscule: far less, for instance, than the probability that a random cosmic ray will sabotage the computer during the computation!

> **Carmichael numbers**
>
> The smallest Carmichael number is $561$. It is not a prime: $561 = 3 \cdot 11 \cdot 17$; yet it fools the Fermat test, because $a^{560} \equiv 1 \bmod{561}$ for all values of a relatively prime to $561$. For a long time it was thought that there might be only finitely many numbers of this type; now we know they are infinite, but exceedingly rare.
>
> There is a way around Carmichael numbers, using a slightly more refined primality test due to Rabin and Miller. Write $N - 1$ in the form $2^t u$. As before we'll choose a random base $a$ and check the value of $a^{N-1} \bmod{N}$. Perform this computation by first determining $a^{u} \bmod{N} and then repeatedly squaring, to get the sequence: $$a^u \bmod{N}, a^{2u} \bmod{N}, \cdots, a^{2^t u} = a^{N-1} \bmod{N}.$$
>
> If $a^{N-1} \not\equiv 1 \bmod{N}$, then $N$ is composite by Fermat's little theorem, and we're done. But if $a^{N-1} \equiv 1 \bmod{N}$, we conduct a little follow-up test: somewhere in the preceding sequence, we ran into a 1 for the first time. If this happened after the first position (that is, if $a^u \bmod{N} \neq 1$), and if the preceding value in the list is not $-1 \bmod{N}, then we declare $N$ composite.
>
> In the latter case, we have found a nontrivial square root of $1 \bmod{N}$: a number that is not $\pm 1 \bmod{N}$ but that when squared is equal to $1 \bmod{N}$. Such a number can only exist if $N$ is composite (Exercise 1.40). It turns out that if we combine this square-root check with our earlier Fermat test, then at least three-fourths of the possible values of a between $1$ and $N - 1$ will reveal a composite $N$, even if it is a Carmichael number.


### 1.3.1 Generating Random Primes

We are now close to having all the tools we need for cryptographic applications. The final piece of the puzzle is a fast algorithm for choosing random primes that are a few hundred bits long. What makes this task quite easy is that primes are abundant -- a random $n$-bit number has roughly a one-in-$n$ chance of being prime (actually about $1 / (\ln{(2^n)}) \approx 1.44 / n$). For instance, *about 1 in 20 social security numbers is prime!*

**Langrange's Prime Number Theorem**
Let $\pi(x)$ be the number of primes $\leq x$. Then $\pi(x) \approx x / \ln{x}$, or more precisely,

$$
\lim_{x \rightarrow \infty} \frac{\pi(x)}{(x / \ln{x})} = 1.
$$

Such abundance makes it simple to generate a random n-bit prime:

+ Pick a random $n$-bit number $N$.

+ Run a primality test on $N$.

+ If it passes the test, output $N$; else repeat the process.


How fast is this algorithm? If the randomly chosen $N$ is truly prime, which happens with probability at least $1 / n$, then it will certainly pass the test. So on each iteration, this procedure has at least a $1 / n$ chance of halting. Therefore on average it will halt within $O(n)$ rounds (Exercise 1.34).

Next, exactly which primality test should be used? In this application, since the numbers we are testing for primality are chosen at random rather than by an adversary, it is sufficient to perform the Fermat test with base $a = 2$ (or to be really safe, $a = 2, 3, 5$), because for random numbers the Fermat test has a much smaller failure probability than the worst-case $1 / 2$ bound that we proved earlier.

Numbers that pass this test have been jokingly referred to as "industrial grade primes." The resulting algorithm is quite fast, generating primes that are hundreds of bits long in a fraction of a second on a PC.

The important question that remains is: what is the probability that the output of the algorithm is really prime? To answer this we must first understand how discerning the Fermat test is. As a concrete example, suppose we perform the test with base $a = 2$ for all numbers $N \leq 25 \times 10^9$.

In this range, there are about $10^9$ primes, and about $20,000$ composites that pass the test (see the following figure). Thus the chance of erroneously outputting a composite is approximately $20,000 / 10^9 = 2 \times 10^{-5}$. This chance of error decreases rapidly as the length of the numbers involved is increased (to the few hundred digits we expect in our applications).


> **Randomized algorithms: a virtual chapter**
>
> Surprisingly â€“ almost paradoxically â€“ some of the fastest and most clever algorithms we have rely on chance: at specified steps they proceed according to the outcomes of random coin tosses. These randomized algorithms are often very simple and elegant, and their output is correct with high probability. This success probability does not depend on the randomness of the input; it only depends on the random choices made by the algorithm itself.
>
> Instead of devoting a special chapter to this topic, in this book we intersperse randomized algorithms at the chapters and sections where they arise most naturally. Furthermore, no specialized knowledge of probability is necessary to follow what is happening. You just need to be familiar with the concept of probability, expected value, the expected number of times we must flip a coin before getting heads, and the property known as "linearity of expectation."
>
> Here are pointers to the major randomized algorithms in this book: One of the earliest and most dramatic examples of a randomized algorithm is the randomized primality test of Figure 1.8. Hashing is a general randomized data structure that supports inserts, deletes, and lookups and is described later in this chapter, in Section 1.5. Randomized algorithms for sorting and median finding are described in Chapter 2. A randomized algorithm for the min cut problem is described in the box on page 143. Randomization plays an important role in heuristics as well; these are described in Section 9.3. And finally the quantum algorithm for factoring (Section 10.7) works very much like a randomized algorithm, its output being correct with high probability â€“ except that it draws its randomness not from coin tosses, but from the superposition principle in quantum mechanics.
>
> Virtual exercises: 1.29, 1.34, 2.24, 9.8, 10.8.
