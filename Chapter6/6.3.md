## 6.3 Edit Distance

When a spell checker encounters a possible misspelling, it looks in its dictionary for other words that are close by. What is the appropriate notion of closeness in this case?

A natural measure of the distance between two strings is the extent to which they can be *aligned*, or matched up. Technically, an alignment is simply a way of writing the strings one above the other. For instance, here are two possible alignments of $\texttt{SNOWY}$ and $\texttt{SUNNY}$:

$$\begin{array}{c c c c}
\begin{matrix}
\texttt{S} & - & \texttt{N} & \texttt{O} & \texttt{W} & \texttt{Y} \\
\texttt{S} & \texttt{U} & \texttt{N} & \texttt{N} & - & \texttt{Y} \\
\end{matrix} &&&
\begin{matrix}
- & \texttt{S} & \texttt{N} & \texttt{O} & \texttt{W} & - & \texttt{Y} \\
\texttt{S} & \texttt{U} & \texttt{N} & - & - & \texttt{N} & \texttt{Y} \\
\end{matrix} \\
\text{Cost: 3} &&& \text{Cost: 5}
\end{array}$$

The "-" indicates a "gap"; any number of these can be placed in either string. The cost of an alignment is the number of columns in which the letters differ. And the *edit distance* between two strings is the cost of their best possible alignment. Do you see that there is no better alignment of $\texttt{SNOWY}$ and $\texttt{SUNNY}$ than the one shown here with a cost of $3$?

Edit distance is so named because it can also be thought of as the minimum number of *edits*—insertions, deletions, and substitutions of characters—needed to transform the first string into the second. For instance, the alignment shown on the left corresponds to three edits: insert $\texttt{U}$, substitute $\texttt{O} \rightarrow \texttt{N}$, and delete $\texttt{W}$.

In general, there are so many possible alignments between two strings that it would be terribly inefficient to search through all of them for the best one. Instead we turn to dynamic programming.

&nbsp;


### A Dynamic Programming Solution

When solving a problem by dynamic programming, the most crucial question is, What are the *subproblems*? As long as they are chosen so as to have the property $\cdot$ from Chapter 6.2. it is an easy matter to write down the algorithm: iteratively solve one subproblem after the other, in order of increasing size.

Our goal is to find the edit distance between two strings $x[1 \cdots m]$ and $y[1 \cdots n]$. What is a good subproblem? Well, it should go part of the way toward solving the whole problem; so how about looking at the edit distance between some *prefix* of the first string, $x[1 \cdots i]$, and some *prefix* of the second, $y[1 \cdots j]$? Call this subproblem $E(i, j)$ (see Figure 6.3). Our final objective, then, is to compute $E(m, n)$.

&nbsp;

**Figure 6.3** The subproblem $E(7, 5)$.

$$\begin{array}{l}
\boxed{\text{E X P O N E N}} \text{T I A L} \\
                                            \\
\boxed{\text{P O L Y N}}   \text{O M I A L} \\
\end{array}$$

&nbsp;


For this to work, we need to somehow express $E(i, j)$ in terms of smaller subproblems. Let's see—what do we know about the best alignment between $x[1 \cdots i]$ and $y[1 \cdots j]$? Well, its rightmost column can only be one of three things:

$$\begin{matrix}
\begin{matrix} x[i] \\ - \end{matrix} && \text{or} && \begin{matrix} - \\ y[j] \end{matrix} && \text{or} && \begin{matrix} x[i] \\ y[j] \end{matrix}
\end{matrix}$$

&nbsp;

The first case incurs a cost of $1$ for this particular column, and it remains to align $x[1 \cdots i - 1]$ with $y[1 \cdots j]$. But this is exactly the subproblem $E(i - 1, j)$! We seem to be getting somewhere.

In the second case, also with cost $1$, we still need to align $x[1 \cdots i]$ with $y[1 \cdots j - 1]$. This is again another subproblem, $E(i, j - 1)$.

And in the final case, which either costs $1$ (if $x[i] \neq y[j]$) or $0$ (if $x[i] = y[j]$), what's left is the subproblem $E(i - 1, j - 1)$. In short, we have expressed $E(i, j)$ in terms of three smaller subproblems $E(i - 1, j), E(i, j - 1), E(i - 1, j - 1)$.

We have no idea which of them is the right one, so we need to try them all and pick the best:
$$E(i, j) = \min\{1 + E(i - 1, j), 1 + E(i, j - 1), \text{diff}(i, j) + E(i - 1, j - 1)\}$$

where for convenience $\text{diff}(i, j)$ is defined to be $0$ if $x[i] = y[j]$ and $1$ otherwise.

For instance, in computing the edit distance between $\texttt{EXPONENTIAL}$ and $\texttt{POLYNOMIAL}$, subproblem $E(4, 3)$ corresponds to the prefixes $\texttt{EXPO}$ and $\texttt{POL}$. The rightmost column of their best alignment must be one of the following:

$$\begin{matrix}
\begin{matrix} \texttt{O} \\ - \end{matrix} && \text{or} && \begin{matrix} - \\ \texttt{L} \end{matrix} && \text{or} && \begin{matrix} \texttt{O} \\ \texttt{L} \end{matrix}
\end{matrix}$$

&nbsp;

Thus, $E(4, 3) = \min\{1 + E(3, 3), 1 + E(4, 2), 1 + E(3, 2)\}$.

The answers to all the subproblems $E(i, j)$ form a two-dimensional table, as in Figure 6.4. In what order should these subproblems be solved? Any order is fine, as long as $E(i - 1, j), E(i, j - 1),$ and $E(i - 1, j - 1)$ are handled *before* $E(i, j)$.

For instance, we could fill in the table one row at a time, from top row to bottom row, and moving left to right across each row. Or alternatively, we could fill it in column by column. Both methods would ensure that by the time we get around to computing a particular table entry, all the other entries we need are already filled in.

![**Figure 6.4** (a) The table of subproblems. Entries $E(i - 1, j - 1), E(i - 1, j),$ and $E(i, j - 1)$ are needed to fill in $E(i, j)$. (b) The final table of values found by dynamic programming.](fig-6.4-edit-distance-table-example.png)

&nbsp;

With both the subproblems and the ordering specified, we are almost done. There just remain the "base cases" of the dynamic programming, the very smallest subproblems. In the present situation, these are $E(0, \cdot)$ and $E(\cdot, 0)$, both of which are easily solved. $E(0, j)$ is the edit distance between the $0$-length prefix of $x$, namely the empty string, and the first $j$ letters of $y$: clearly, $j$. And similarly, $E(i, 0) = i$.

At this point, the algorithm for edit distance basically writes itself.

```python
def edit_distance(word_1, word_2):
  """
  Input: word_1 of length m, word_2 of length n
  Output: edit distance between word_1 and word_2
  """
  for i = 0, ..., m:
    E(i, 0) = i
  for j = 1, ..., n:
    E(0, j) = j
  for i = 1, ..., m:
    for j = 1, ..., n:
      E(i, j) = min(1 + E(i - 1, j), 1 + E(i, j - 1), diff(i, j) + E(i - 1, j - 1))
  return E(m, n)
```

This procedure fills in the table row by row, and left to right within each row. Each entry takes constant time to fill in, so the overall running time is just the size of the table, $O(m n)$.

&nbsp;

And in our example, the edit distance turns out to be $6$:
$$\begin{matrix}
\begin{matrix} \texttt{E} \\ -          \end{matrix} &
\begin{matrix} \texttt{X} \\ -          \end{matrix} &
\begin{matrix} \texttt{P} \\ \texttt{P} \end{matrix} &
\begin{matrix} \texttt{O} \\ \texttt{O} \end{matrix} &
\begin{matrix} \texttt{N} \\ \texttt{L} \end{matrix} &
\begin{matrix} \texttt{E} \\ \texttt{Y} \end{matrix} &
\begin{matrix} \texttt{N} \\ \texttt{N} \end{matrix} &
\begin{matrix} -          \\ \texttt{O} \end{matrix} &
\begin{matrix} \texttt{T} \\ \texttt{M} \end{matrix} &
\begin{matrix} \texttt{I} \\ \texttt{I} \end{matrix} &
\begin{matrix} \texttt{A} \\ \texttt{A} \end{matrix} &
\begin{matrix} \texttt{L} \\ \texttt{L} \end{matrix} &
\end{matrix}$$

&nbsp;


### The underlying DAG

Every dynamic program has an underlying $\text{DAG}$ structure: think of each node as representing a subproblem, and each edge as a precedence constraint on the order in which the subproblems can be tackled. Having nodes $u_1, \ldots, u_k$ point to $v$ means "subproblem $v$ can only be solved once the answers to $u_1, \ldots, u_k$ are known."

In our present edit distance application, the nodes of the underlying $\text{DAG}$ correspond to subproblems, or equivalently, to positions $(i, j)$ in the table. Its edges are the precedence constraints, of the form $(i - 1, j) \rightarrow (i, j), (i, j - 1) \rightarrow (i, j)$, and $(i - 1, j - 1) \rightarrow (i, j)$ (Figure 6.5). In fact, we can take things a little further and put weights on the edges so that the edit distances are given by shortest paths in the $\text{DAG}$!

To see this, set all edge lengths to $1$, except for $\{(i - 1, j - 1) \rightarrow (i, j) : x[i] = y[j]\}$ (shown dotted in the figure), whose length is $0$. The final answer is then simply the distance between nodes $s = (0, 0)$ and $t = (m, n)$. One possible shortest path is shown, the one that yields the alignment we found earlier. On this path, each move down is a deletion, each move right is an insertion, and each diagonal move is either a match or a substitution.

![**Figure 6.5** The underlying $\text{DAG}$, and a path of length $6$.](fig-6.5-edit-distance-underlying-dag.png)

&nbsp;


By altering the weights on this $\text{DAG}$, we can allow generalized forms of edit distance, in which insertions, deletions, and substitutions have different associated costs.

&nbsp;


> **Common Subproblems**
>
> Finding the right subproblem takes creativity and experimentation. But there are a few standard choices that seem to arise repeatedly in dynamic programming.
>
> 1. The input is $x_1, x_2, \ldots, x_n$ and a subproblem is $x_1, x_2, \ldots, x_i$. $$\begin{array}{l}
\boxed{\text{$x_1$ \hspace{0.05cm} $x_2$ \hspace{0.05cm} $x_3$ \hspace{0.05cm} $x_4$ \hspace{0.05cm} $x_5$ \hspace{0.05cm} $x_6$}} \text{\hspace{0.05cm} $x_7$ \hspace{0.05cm} $x_8$ \hspace{0.05cm}  $x_9$ \hspace{0.05cm} $x_{10}$} \end{array}$$ The number of subproblems is therefore linear $O(n)$.
>
> 2. The input is $x_1, \ldots, x_n$, and $y_1, \ldots, y_m$. A subproblem is $x_1, \ldots, x_i$ and $y_1, \ldots, y_j$. $$\begin{array}{l}
\boxed{\text{$x_1$ \hspace{0.05cm} $x_2$ \hspace{0.05cm} $x_3$ \hspace{0.05cm} $x_4$ \hspace{0.05cm} $x_5$ \hspace{0.05cm} $x_6$}} \text{\hspace{0.05cm} $x_7$ \hspace{0.05cm} $x_8$ \hspace{0.05cm}  $x_9$ \hspace{0.05cm} $x_{10}$} \\ \\
\boxed{\hspace{0.03cm} \text{$y_1$ \hspace{0.08cm} $y_2$ \hspace{0.08cm} $y_3$ \hspace{0.08cm} $y_4$ \hspace{0.08cm} $y_5$ \hspace{0.07cm} $y_6$}} \text{\hspace{0.08cm} $y_7$ \hspace{0.08cm} $y_8$}
\end{array}$$ The number of subproblems is $O(mn)$.
>
> 3. The input is $x_1, \ldots, x_n$ and a subproblem is $x_i, x_{i+1}, \ldots, x_j$. $$\begin{array}{l} \text{$x_1$ \hspace{0.05cm} $x_2$ \hspace{0.05cm}} \boxed{\text{$x_3$ \hspace{0.05cm} $x_4$ \hspace{0.05cm} $x_5$ \hspace{0.05cm} $x_6$}} \text{\hspace{0.05cm} $x_7$ \hspace{0.05cm} $x_8$ \hspace{0.05cm}  $x_9$ \hspace{0.05cm} $x_{10}$}
\end{array}$$ The number of subproblems $O(n^2)$.
>
> 4. The input is a rooted tree. A subproblem is a rooted subtree. ![](rooted-subtree-subproblem.png) If the tree has $n$ nodes, how many subproblems are there?
>
> We've already encountered the first two cases, and the others are coming up shortly.

&nbsp;


> **Of Mice and Men**
>
> Our bodies are extraordinary machines: flexible in function, adaptive to new environments, and able to interact and reproduce. All these capabilities are specified by a program unique to each of us, a string that is $3$ billion characters long over the alphabet $\{A, C, G, T\}$—our DNA.
>
> The DNA sequences of any two people differ by only about $0.1\%$. However, this still leaves $3$ million positions on which they vary, more than enough to explain the vast range of human diversity. These differences are of great scientific and medical interest—for instance, they might help predict which people are prone to certain diseases.
>
> DNA is a vast and seemingly inscrutable program, but it can be broken down into smaller units that are more specific in their role, rather like subroutines. These are called genes. Computers have become a crucial tool in understanding the genes of humans and other organisms, to the extent that [computational genomics](https://en.wikipedia.org/wiki/Computational_genomics) is now a field in its own right. Here are examples of typical questions that arise.
>
> 1. When a new gene is discovered, one way to gain insight into its function is to find known genes that match it closely. This is particularly helpful in transferring knowledge from well-studied species, such as mice, to human beings.
>  A basic primitive in this search problem is to define an efficiently computable notion of when two strings approximately match. The biology suggests a generalization of edit distance, and dynamic programming can be used to compute it.
> Then there's the problem of searching through the vast thicket of known genes: the database GenBank already has a total length of over $10^{10}$, and this number is growing rapidly. The current method of choice is BLAST, a clever combination of algorithmic tricks and biological intuitions that has made it the most widely used software in computational biology.
>
> 2. Methods for *sequencing* DNA (that is, determining the string of characters that constitute it) typically only find fragments of $500–700$ characters. Billions of these randomly scattered fragments can be generated, but how can they be assembled into a coherent DNA sequence? For one thing, the position of any one fragment in the final sequence is unknown and must be inferred by piecing together overlapping fragments.
> A showpiece of these efforts is the draft of human DNA completed in 2001 by two groups simultaneously: the publicly funded Human Genome Consortium and the private Celera Genomics.
>
> 3. When a particular gene has been sequenced in each of several species, can this information be used to reconstruct the evolutionary history of these species?
>
> We will explore these problems in the exercises at the end of this chapter. Dynamic programming has turned out to be an invaluable tool for some of them and for computational biology in general.

&nbsp;
