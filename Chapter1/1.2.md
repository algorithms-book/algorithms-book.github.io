## 1.2 Modular Arithmetic

With repeated addition or multiplication, numbers can get cumbersomely large. So it is fortunate that we reset the hour to zero whenever it reaches 24, and the month to January after every stretch of 12 months. Similarly, for the built-in arithmetic operations of computer processors, numbers are restricted to some size, 32 bits say, which is considered generous enough for most purposes.

For the applications we are working toward – primality testing and cryptography – it is necessary to deal with numbers that are significantly larger than 32 bits, but whose range is nonetheless limited.

**Modular arithmetic** is a system for dealing with restricted ranges of integers. We define $x \pmod{N}$ to be the remainder when $x$ is divided by $N$; that is, if $x = qN + r$ with $0 \leq r < N$, then $x \pmod{N}$ is equal to $r$. This gives an enhanced notion of **equivalence** between numbers: $x$ and $y$ are *congruent* $\pmod{N}$ if they differ by a multiple of $N$, or in symbols,

$$
x \equiv y \pmod{N} \ \Longleftrightarrow\ N \ \text{divides}\ (x - y).
$$

For instance $256 \equiv 13 \pmod{60}$ because $253 - 13 = 240$ is a multiple of $60$; more familiarly, $253$ minutes is $4$ hours and $13$ minutes. These numbers can also be negative, as in $59 \equiv -1 \pmod{60}$: when it is $59$ minutes past the hour, it is also $1$ minute short of the next hour.

One way to think of modular arithmetic is that it limited numbers to a predefined range $\{ 0, 1, \cdots, N - 1 \}$ and wraps around whenever you try to leave this range – like the hand of a clock (Figure 1.3).

![**Figure 1.3** Addition modulo 8.](fig-1.3-addition-modulo-8.png)

Another interpretation is that modular arithmetic deals with all the integers, but divides them into $N$ **equivalence classes**, each of the form $\{ i + k N : k \in \mathbb{Z} \}$ for some $i$ between $0$ and $N - 1$. For example, there are three equivalence classes modulo $3$:

$$
\begin{matrix}
\cdots & -9 & -6 & -3 & 0 & 3 & 6 & 9  & \cdots \\
\cdots & -8 & -5 & -2 & 1 & 4 & 7 & 10 & \cdots \\
\cdots & -7 & -4 & -1 & 2 & 5 & 8 & 11 & \cdots \\
\end{matrix}
$$

Any member of an equivalence class is substitutable for each other; when viewed modulo $3$, the numbers $5$ and $11$ are no different. Under such substitutions, addition and multiplication remain well-defined:

**Substitution rule**
If $x \equiv x' \pmod{N}$ and $y \equiv y' \pmod{N}$, then: $$x + y \equiv x' + y' \pmod{N}$$ and $$xy \equiv x'y' \pmod{N}.$$

For instance, suppose you watch an entire season of your favorite television show in one sitting, starting at midnight. There are $25$ episodes, each last $3$ hours. At which time of the day are you done? Answer: the hour of completion is $(25 \times 3) \pmod{24}$, which since $24 \equiv 1 \pmod{24}$ is $1 \times 3 \equiv 3 \pmod{24}$, or three o'clock in the morning.

It is not hard to check that in modular arithmetic, the usual associative, commutative, and distributive properties of addition and multiplication continue to apply, for instance:

$$
\begin{aligned}
x + (y + z) &\equiv (x + y) + z \pmod{N} & \text{associativity} \\
xy &\equiv yx \pmod{N}                   & \text{commutativity} \\
x(y + z) &\equiv xy + yz \pmod{N}        & \text{distributivity}\\
\end{aligned}
$$

Taken together with the substitution rule, this implies that while performing a sequence of arithmetic operations, it is legal to reduce intermediate results to their remainders modulo $N$ at any stage. Such simplifications can be a dramatic help in big calculations.

Witness, for instance:

$$
2^{345} \equiv (2^5)^{69} \equiv 32^{69} \equiv 1^{69} \equiv 1 \pmod{31}.
$$

&nbsp;

### 1.2.1 Modular Addition and Multiplication

To *add* two numbers $x$ and $y$ modulo $N$, we start with regular addition. Since $x$ and $y$ are each in the range $0$ to $N - 1$, their sum is between $0$ and $2(N - 1)$. If the sum exceeds $N - 1$, we merely subtract off $N$ to bring it back into the required range. The overall computation therefore consists of an addition, and possibly a subtraction, of numbers that never exceed $2N$. Its running time is linear in the sizes of these numbers, in other words $O(n)$, where $n = \log N$ is hte size of $N$. As a reminder, our convention is to use the letter $n$ to denote input size.

To *multiply* two numbers $x$ and $y$ modulo $N$, we again start with regular multiplication and then reduce the answer modulo $N$. The product can be as large as $(N - 1)^2$, but this is still at most $2n$ bits long since $\log (N - 1)^2 = 2\log (N - 1) \leq 2n$. To reduce the answer modulo $N$, we compute the remainder upon dividing it by $N$, using our quadratic-time division algorithm. Multiplication thus remains a quadratic operation.

&nbsp;

> **Two's Complement**
>
> Modular arithmetic is nicely illustrated in *two’s complement*, the most common format for storing signed integers. It uses $n$ bits to represent numbers in the range $[-2^{n-1}, 2^{n-1} - 1]$ and is usually described as follows:
>
> * Positive integers, in the range $0$ to $2^{n-1} - 1$, are stored in regular binary and have a leading bit of $0$.
>
> * Negative integers $-x$, with $1 \leq x \leq 2^{n-1}$, are stored by first constructing $x$ in binary, then flipping all the bits, and finally adding $1$. The leading bit in this case is $1$.
>
> (And the usual description of addition and multiplication in this format is even more arcane!)
>
> Here’s a much simpler way to think about it: any number in the range $-2^{n-1}$ to $2^{n-1} - 1$ is stored modulo $2^n$. Negative numbers $-x$ therefore end up as $2^{n} - x$. Arithmetic operations like addition and subtraction can be performed directly in this format, ignoring any overflow bits that arise.

&nbsp;

*Division* is not quite so easy. In ordinary arithmetic, there is just one tricky case – division by zero. It turns out that in modular arithmetic there are potentially other such cases as well, which we will characterize toward the end of this section. Whenever division is legal, however, it can be managed in cubic time $O(n^3)$.

To complete the suite of modular arithmetic primitives we need for cryptography, we next turn to modular exponentiation, and then to the greatest common divisor, which is the key to division. For both tasks, the most obvious procedures take exponentially long, but with some ingenuity, polynomial-time solutions can be found. A careful choice of algorithms makes all the difference.

&nbsp;

### 1.2.2 Modular Exponentiation

In the cryptosystem we are working toward, it is necessary to compute $x^y \pmod{N}$ for values of $x$, $y$, and $N$ that are several hundred bits long. Can this be done quickly?

The result is some number modulo $N$ and is therefore itself a few hundred bits long. However, the raw value of $xy$ could be much, much longer than this. Even when $x$ and $y$ are just $20$-bit numbers, $x^y$ is at least $(2^{19})^{(2^{19})} = 2^{(19)(524288)}$, about $10$ million bits long! Imagine what happens if $y$ is a $500$-bit number!

To make sure the numbers we are dealing with never grow too large, we need to perform all intermediate computations modulo $N$. So here’s an idea: calculate $x^y \pmod{N}$ by repeatedly multiplying by $x \pmod{N}$. The resulting sequence of intermediate products,

$$
x \pmod{N} \longrightarrow x^2 \pmod{N} \longrightarrow x^3 \pmod{N} \longrightarrow \ \cdots\ \longrightarrow x^y \pmod{N},
$$

consists of numbers that are smaller than $N$, and so the individual multiplications do not take too long. But there's a problem: if $y$ is $500$ bits long, we need to perform $y - 1 \approx 2^{500}$ multiplications! This algorithm is clearly exponential in the size of $y$.

Luckily, we can do better: starting with $x$ and **squaring repeatedly** modulo $N$, we get

$$
x \pmod{N} \longrightarrow x^2 \pmod{N} \longrightarrow x^4 \pmod{N} \longrightarrow x^8 \pmod{N} \longrightarrow \ \cdots\ \longrightarrow x^{2 \log{y}} \pmod{N}.
$$

Each takes just $O(\log^2 N)$ time to compute, and in this case there are only $\log{y}$ multiplications. To determine $x^y \pmod{N}$, we simply multiply together an appropriate subset of these powers, those corresponding to $1$'s in the binary representation of $y$.

For instance,

$$
x^{25} = x^{11001_2} = x^{10000_2} \cdot x^{1000_2} \cdot x^{1_2} = x^{16} \cdot x^8 \cdot x^{1}.
$$

A polynomial-time algorithm is finally within reach!

We can package this idea in a particularly simply form: the recursive algorithm of Figure 1.4, which works by executing the self-evident rule

$$
x^y = \begin{cases} (x^{y / 2})^2 & \text{if $y$ is even} \\ (x^{y / 2})^2 \cdot x & \text{if $y$ is odd} \end{cases}
$$


**Figure 1.4** Modular exponentiation

```python
def modexp(x, y, N):
    """
    Input: two n-bit integers x and N, an integer exponent y
    Output: x^y mod N
    """

    if y == 0:
        return 1

    z = modexp(x, y / 2, N)
    if y is even:
        return z^2 mod N
    else:
        return x * z^2 mod N
```

In doing so, it closely parallels our recursive multiplication algorithm (Figure 1.1). For instance, that algorithm would compute the product $x \cdot 25$ to the analogous decomposition to one we saw $x \cdot 25 = x \cdot 16 + x \cdot 8 + x \cdot 1$. And whereas for multiplication for the terms $x \cdot 2^{i}$ come from repeated doubling, for exponentation the corresponding terms $x^{2^i}$ are generated by repeated squaring.

Let $n$ be the size in bits of $x, y$, and $N$ (whichever is the largest of the three). As with multiplication, the algorithm will halt after at most $n$ recursive calls, and during each call it multiplies $n$-bit numbers (doing computation modulo $N$ saves us here), for a total running time of $O(n^3)$.

&nbsp;

### 1.2.3 Euclid's Algorithm for Greatest Common Divisor

Our next algorithm was discovered well over 2000 years ago by the mathematician Euclid, in ancient Greece. Given two integers $a$ and $b$, it finds the largest integer that divides both of them, known as their **greatest common divisor (gcd)**.

```python
def Euclid(a, b):
    """
    Input: two integers a and b with a >= b >= 0
    Output: gcd(a, b)
    """
    if b == 0:
        return a
    else:
        return Euclid(b, a mod b)
```

The most obvious approach is to first factor $a$ and $b$, then multiply their common factors. For instance, $1035 = 3^2 \cdot 5 \cdot 23$ and $759 = 3 \cdot 11 \cdot 23$, so their $\text{gcd}$ is $3 \cdot 23 = 69$. However, we have no efficient algorithm for factoring. Is there some other way to compute the greatest common divisors?

Euclid's algorithm uses the following simple formula.

**Euclid's rule**
If $x$ and $y$ are positive integers with $x \geq y$, then $\text{gcd}(x, y) = \text{gcd}(y, x \pmod{y})$.

*Proof.* It is enough to show the slightly simpler rule $\text{gcd}(x, y) = \text{gcd}(y, x - y)$ from which the one stated can be derivated by repeatedly subtracting $y$ from $x$. Here it goes. Any integer that divides both $x$ and $y$ must also divide $x - y$, so $\text{gcd}(x, y) \leq \text{gcd}(y, x - y)$. Likewise, any integer that divides $x - y$ and $y$ must also divide both $x$ and $y$ (since $x - y + y = x$), so $\text{gcd}(x, y) \geq \text{gcd}(y, x - y)$. $\blacksquare$

Euclid’s rule allows us to write down an elegant recursive algorithm (Figure 1.5), and its correctness follows immediately from the rule. In order to figure out its running time, we need to understand how quickly the arguments $(a, b)$ decrease with each successive recursive call. In a single round, arguments $(a, b)$ become $(b, a \pmod{b})$: their order is swapped, and the larger of them, $a$, gets reduced to $a \pmod{b}$. This is a substantial reduction.


**Lemma**
If $a \geq b$, then $a \pmod{b} < \frac{a}{2}$.

*Proof.* Witness that either $b \leq \frac{a}{2}$ or $b > \frac{a}{2}$. These two case are shown in the following figure. If $b leq \frac{a}{2}$, then we have $a \pmod{b} < b \leq \frac{a}{2}$. And if $b > \frac{a}{2}$, then $a \pmod{b} = a - b < \frac{a}{2}$. $\blacksquare$

![](euclids-rule.png)

This means that after any two consecutive rounds, both arguments, $a$ and $b$, are at the very least halved in value – the length of each decreases by at least one bit. If they are initially $n$-bit integers, then the base case will be reached within $2n$ recursive calls. And since each call involves a quadratic-time division, the total time is $O(n^3)$.

&nbsp;

### 1.2.4 An Extension of Euclid's Algorithm

```python
def extended_Euclid(a, b):
    """
    Input: two integers a and b with a >= b >= 0
    Output: integers x, y, d, such that d = gcd(a, b) and ax + by = d
    """
    if b == 0:
        return (1, 0, a)
    else:    
        (x, y, d) = extended_Euclid(b, a mod b)
        return (y, x - (a / b)y, d)
```

A small extension to Euclid’s algorithm is the key to dividing in the modular world. To motivate it, suppose someone claims that $d$ is the greatest common divisor of $a$ and $b$: how can we check this? It is not enough to verify that $d$ divides both $a$ and $b$, because this only shows $d$ to be a common factor, not necessarily the largest one. Here’s a test that can be used if $d$ is of a particular form.

**Lemma**
If $d$ divides both $a$ and $b$, and $d = ax + by$ for some integers $x$ and $y$, then necessarily $d = \text{gcd}(a, b)$.

*Proof.*
By the first two conditions, $d$ is a common divisor of $a$ and $b$ and so it cannot exceed the greatest common divisor; that is, $d \leq \text{gcd}(a, b)$. On the other hand, since $\text{gcd}(a, b)$ is a common divisor of $a$ and $b$, it must also divide $ax + by = d$, which implies $\text{gcd}(a, b) \leq d$. Putting these together, $d = \text{gcd}(a, b)$. $\blacksquare$

**Lemma**
For any positive integers $a$ and $b$, the extended Euclid algorithm returns integers $x, y,$ and $d$ such that $\text{gcd}(a, b) = d = ax + by$.

*Proof.*
The first thing to confirm is that if you ignore the $x$ and $y$, the extended algorithm is exactly the same as the original. So, at least we compute $d = \text{gcd}(a, b)$. For the rest, the recursive nature of the algorithm suggests a proof by induction. The recursion ends when $b = 0$, so it is convenient to do induction on the value of $b$. The base case $b = 0$ is easy enough to check directly.

Now pick any larger value of $b$. The algorithm finds $\text{gcd}(a, b)$ by calling $\text{gcd}(b, a \pmod{b})$. Since $a \pmod{b} < b$, we can apply the inductive hypothesis to this recursive call and conclude that the $x$ and $y$ it returns are correct:

$$
\text{gcd}(b, a \pmod{b}) = bx + (a \pmod{b})y.
$$

Writing $a \pmod{b}$ as $a - (a / b)b$, we find

$$
d = \text{gcd}(a, b) = \text{gcd}(b, a \pmod{b}) = bx + (a \pmod{b})y = bx + (a - (a / b)b)y = ay + b(x - (a / b)y).
$$

Therefore $d = ax + by$ with $x = y$ and $y = x - (a / b)y$, thus validating the algorithm's behavior on input $(a, b)$. $\blacksquare$

For example, to compute $\text{gcd}(25, 11)$, Euclid's algorithm would proceed as follows:

$$
\begin{aligned}
\underline{25} &= 2 \cdot \underline{11} + 3 \\
\underline{11} &= 3 \cdot \underline{ 3} + 2 \\
\underline{ 3} &= 1 \cdot \underline{ 2} + 1 \\
\underline{ 2} &= 2 \cdot \underline{ 1} + 0 \\
\end{aligned}
$$

(at each stage, the $\text{gcd}$ computation has been reduced to the underlined numbers).

Thus, $\text{gcd}(25, 11) = \text{gcd}(11, 3) = \text{gcd}(3, 2) = \text{gcd}(1, 0)$.

To find $x$ and $y$ such that $25x + 11y = 1$, we start by expressing $1$ in terms of the last pair $(1, 0)$. Then we work backwards and express it in terms of $(2, 1), (3, 2), (11, 3)$, and finally $(25, 11)$. The first step is:

$$
1 = \underline{1} - \underline{0}.
$$

To rewrite this in terms of $(2, 1)$, we use the substitution $0 = 2 - 2 \cdot 1$ from the last line of the gcd calculation to get:

$$
1 = \underline{1} - (\underline{2} - 2 \cdot \underline{1}) = -1 \cdot \underline{2} + 3 \cdot \underline{1}.
$$

The second-last line of the gcd calculation tells us that $1 = 3 - 1 \cdot 2$. Substituting:

$$
1 = -1 \cdot \underline{2} + 3(\underline{3} - 1 \cdot \underline{2}) = 3 \cdot \underline{3} - 4 \cdot \underline{2}.
$$

Continuing in this same way with substitutions $2 = 11 - 3 \cdot 3$ and $3 = 25 - 2 \cdot 11$ gives:

$$
1 = 3 \cdot \underline{3} - 4(\underline{11} - 3 \cdot \underline{3}) = -4 \cdot \underline{11} + 15 \cdot \underline{3} = -4 \cdot \underline{11} + 15(\underline{25} - 2 \cdot \underline{11}) = 15 \cdot \underline{25} - 34 \cdot \underline{11}.
$$

We're done: $15 \cdot 25 - 32 \cdot 1 = 1$, so $x = 15$ and $y = -34$.

&nbsp;

### 1.2.5 Modular Division

In real arithmetic, every number $a \neq 0$ has an inverse $\frac{1}{a}$, and dividing by $a$ is the same as multiplying by this inverse. In modular arithmetic, we can make a similar definition.

* We say $x$ is the **multiplicative inverse** of $a$ modulo $N$ if $ax \equiv 1 \pmod{N}$.

There can be at most one such $x$ modulo $N$, and we shall denote it by $a^{-1}$. However, *this inverse does not always exist!* For instance, $2$ is not invertible modulo $6$: that is, $2x \not\equiv 1 \pmod{6}$ for every possible choice of $x$. In this case, $a$ and $N$ are both even and thus then $a \pmod{N}$ is always even, since $a \pmod{N} = a - kN$ for some $k$.

More generally, we can be certain that $\text{gcd}(a, N)$ divides $ax \pmod{N}$, because this latter quantity can be written in the form $ax + kN$. So if $\text{gcd}(a, N) > 1$ then $ax \not\equiv 1 \pmod{N}$, no matter what $x$ might be, and therefore $a$ cannot have a multiplicative inverse modulo $N$.

In fact, this is the only circumstance in which $a$ is not invertible. When $\text{gcd}(a, N) = 1$ (we say $a$ and $N$ are relatively prime, or co-prime), the extended Euclid algorithm gives us integers $x$ and y such that $ax + Ny = 1$, which means that $ax \equiv 1 \pmod{N}$. Thus $x$ is $a$’s sought inverse.

For example, suppose we wish to compute $11^{-1} \pmod{25}$. Using the extended Euclid algorithm, we find that $15 \cdot 25 - 34 \cdot 11 = 1$. Reducing both sides modulo $25$, we have $-34 \cdot 11 \equiv 1 \pmod{25}$. So $-34 \equiv 16 \pmod{25}$ is the inverse of $11 \pmod{25}$.

**Modular Division Theorem**
For any $a \pmod{N}$, $a$ has a multiplicative inverse modulo $N$ if and only if it is relatively prime to $N$. When this inverse exists, it can be found in time $O(n^3)$ (where as usual $n$ denotes the number of bits of $N$) by running the extended Euclid algorithm.

This resolves the issue of modular division: when working modulo $N$, we can divide by numbers relatively prime to $N$ – and only by these. And to actually carry out the division, we multiply by the inverse.

&nbsp;

> **Is Your Social Security Number a Prime?**
>
> The numbers $7, 17, 19, 71,$ and $79$ are primes, but how about $717-19-7179$? Telling whether a reasonably large number is a prime seems tedious because there are far too many candidate factors to try. However, there are some clever tricks to speed up the process. For instance, you can omit even-valued candidates after you have eliminated the number $2$. You can actually omit all candidates except those that are themselves primes.
>
> In fact, a little further thought will convince you that you can proclaim $N$ a prime as soon as you have rejected all candidates *up to* $\sqrt{N}$, for if $N$ can indeed be factored as $N = K \cdot L$, then it is impossible for both factors to exceed $\sqrt{N}$.
>
> We seem to be making progress! Perhaps by omitting more and more candidate factors, a truly efficient primality test can be discovered.
>
> Unfortunately, there is no fast primality test down this road. The reason is that we have been trying to tell if a number is a prime *by factoring it*. And factoring is a hard problem!
>
> Modern cryptography, as well as the balance of this chapter, is about the following important idea: ***factoring is hard and primality is easy***. We cannot factor large numbers, but we can easily test huge numbers for primality! (Presumably, if a number is composite, such a test will detect this *without finding a factor*.)
