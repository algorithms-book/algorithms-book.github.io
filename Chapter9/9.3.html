<!DOCTYPE html>
<html>
<head>

  <title>9.3 Local Search Heuristics</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="local-search-heuristics">9.3 Local Search Heuristics</h2>
  <p>Our next strategy for coping with <span class="math inline">\(\textbf{NP}\)</span>-completeness is inspired by evolution (which is, after all, the world's best-tested optimization procedure)—by its incremental process of introducing small mutations, trying them out, and keeping them if they work well. This paradigm is called <em>local search</em> and can be applied to any optimization task. Here's how it looks for a minimization problem.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">let s be <span class="bu">any</span> initial solution
  <span class="cf">while</span> there <span class="kw">is</span> some solution s<span class="op">*</span> <span class="kw">in</span> the neighborhood of s
    <span class="cf">for</span> which cost(s<span class="op">*</span>) <span class="op">&lt;</span> cost(s): replace s by s<span class="op">*</span>
  <span class="cf">return</span> s</code></pre></div>
  <p>On each iteration, the current solution is replaced by a better one close to it, in its neighborhood. This neighborhood structure is something we impose upon the problem and is the central design decision in local search. As an illustration, let's revisit the traveling salesman problem.</p>
  <p> </p>
  <h3 id="traveling-salesman-once-more">9.3.1 Traveling Salesman, Once More</h3>
  <p>Assume we have all interpoint distances between <span class="math inline">\(n\)</span> cities, giving a search space of <span class="math inline">\((n - 1)!\)</span> different tours. What is a good notion of neighborhood? The most obvious notion is to consider two tours as being close if they differ in just a few edges. They can't differ in just one edge (do you see why?), so we will consider differences of two edges. We define the <span class="math inline">\(2\)</span>-<em>change</em> neighborhood of tour <span class="math inline">\(s\)</span> as being the set of tours that can be obtained by removing two edges of <span class="math inline">\(s\)</span> and then putting in two other edges. Here's an example of a local move:</p>
  <div class="figure">
  <img src="tsp-local-2-swap-example.png" />

  </div>
  <p>We now have a well-defined local search procedure. How does it measure up under our two standard criteria for algorithms—what is its overall running time, and does it always return the best solution?</p>
  <p>Embarrassingly, neither of these questions has a satisfactory answer. Each iteration is certainly fast, because a tour has only <span class="math inline">\(O(n^2)\)</span> neighbors. However, it is not clear how many iterations will be needed: whether for instance, there might be an exponential number of them.</p>
  <p>Likewise, all we can easily say about the final tour is that it is locally optimal—that is, it is superior to the tours in its immediate neighborhood. There might be better solutions further away. For instance, the following picture shows a possible final answer that is clearly suboptimal; the range of local moves is simply too limited to improve upon it.</p>
  <div class="figure">
  <img src="tsp-local-2-suboptimal-example.png" />

  </div>
  <p>To overcome this, we may try a more generous neighborhood, for instance <span class="math inline">\(3\)</span>-<em>change</em>, consisting of tours that differ on up to three edges. And indeed, the preceding bad case gets fixed:</p>
  <div class="figure">
  <img src="tsp-local-3-swap-example.png" />

  </div>
  <p>But there is a downside, in that the size of a neighborhood becomes <span class="math inline">\(O(n^3)\)</span>, making each iteration more expensive. Moreover, there may still be suboptimal local minima, although fewer than before. To avoid these, we would have to go up to <span class="math inline">\(4\)</span>-<em>change</em>, or higher.</p>
  <p>In this manner, efficiency and quality often turn out to be competing considerations in a local search. Efficiency demands neighborhoods that can be searched quickly, but smaller neighborhoods can increase the abundance of low-quality local optima. The appropriate compromise is typically determined by experimentation.</p>
  <p>Figure 9.7 shows a specific example of local search at work. Figure 9.8 is a more abstract, stylized depiction of local search.</p>
  <div class="figure">
  <img src="fig-9.7-local-search-tsp-example.png" alt="Figure 9.7 (a) Nine American cities. (b) Local search, starting at a random tour, and using 3-change. The traveling salesman tour is found after three moves." />
  <p class="caption"><strong>Figure 9.7</strong> (a) Nine American cities. (b) Local search, starting at a random tour, and using 3-change. The traveling salesman tour is found after three moves.</p>
  </div>
  <div class="figure">
  <img src="fig-9.8-local-search-example.png" alt="Figure 9.8 Local search." />
  <p class="caption"><strong>Figure 9.8</strong> Local search.</p>
  </div>
  <p>The solutions crowd the unshaded area, and cost decreases when we move downward. Starting from an initial solution, the algorithm moves downhill until a local optimum is reached.</p>
  <p>In general, the search space might be riddled with local optima, and some of them may be of very poor quality. The hope is that with a judicious choice of neighborhood structure, most local optima will be reasonable. Whether this is the reality or merely misplaced faith, it is an empirical fact that local search algorithms are the top performers on a broad range of optimization problems. Let's look at another such example.</p>
  <p> </p>
  <h3 id="graph-partitioning">9.3.2 Graph Partitioning</h3>
  <p>The problem of graph partitioning arises in a diversity of applications, from circuit layout to program analysis to image segmentation. We saw a special case of it, <span class="math inline">\(\text{BALANCED CUT}\)</span>, in Chapter 8.</p>
  <p> </p>
  <p><span class="math inline">\(\text{GRAPH PARTITIONING}\)</span></p>
  <ul>
  <li><p>input: an undirected graph <span class="math inline">\(G = (V, E)\)</span> with nonnegative edge weights; a real number <span class="math inline">\(\alpha \in (0, 1/2]\)</span>.</p></li>
  <li><p>output: a partition of the vertices into two groups <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, each of size at least <span class="math inline">\(\alpha|V|\)</span>.</p></li>
  <li><p>goal: minimize the capacity of the cut <span class="math inline">\((A, B)\)</span>.</p></li>
  </ul>
  <p> </p>
  <p>Figure 9.9 shows an example in which the graph has <span class="math inline">\(16\)</span> nodes, all edge weights are <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, and the optimal solution has cost <span class="math inline">\(0\)</span>. Removing the restriction on the sizes of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> would give the <span class="math inline">\(\text{MINIMUM CUT}\)</span> problem, which we know to be efficiently solvable using flow techniques. The present variant, however, is <span class="math inline">\(\textbf{NP}\)</span>-hard. In designing a local search algorithm, it will be a big convenience to focus on the special case <span class="math inline">\(\alpha = 1/2\)</span>, in which <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are forced to contain exactly half the vertices. The apparent loss of generality is purely cosmetic, as <span class="math inline">\(\text{GRAPH PARTITIONING}\)</span> reduces to this particular case.</p>
  <div class="figure">
  <img src="fig-9.9-graph-partitioning-example.png" alt="Figure 9.9 An instance of \text{GRAPH PARTITIONING}, with the optimal partition for \alpha = 1/2. Vertices on one side of the cut are shaded." />
  <p class="caption"><strong>Figure 9.9</strong> An instance of <span class="math inline">\(\text{GRAPH PARTITIONING}\)</span>, with the optimal partition for <span class="math inline">\(\alpha = 1/2\)</span>. Vertices on one side of the cut are shaded.</p>
  </div>
  <p>We need to decide upon a neighborhood structure for our problem, and there is one obvious way to do this. Let (<span class="math inline">\(A, B\)</span>), with <span class="math inline">\(|A| = |B|\)</span>, be a candidate solution; we will define its neighbors to be all solutions obtainable by swapping one pair of vertices across the cut, that is, all solutions of the form <span class="math inline">\((A - \{a\} + \{b\}, B - \{b\} + \{a\})\)</span> where <span class="math inline">\(a \in A\)</span> and <span class="math inline">\(b \in B\)</span>. Here's an example of a local move:</p>
  <div class="figure">
  <img src="graph-partition-swap-example.png" />

  </div>
  <p>We now have a reasonable local search procedure, and we could just stop here. But there is still a lot of room for improvement in terms of the <em>quality</em> of the solutions produced. The search space includes some local optima that are quite far from the global solution. Here's one which has cost <span class="math inline">\(2\)</span>.</p>
  <div class="figure">
  <img src="graph-partition-swap-revisited-example.png" />

  </div>
  <p>What can be done about such suboptimal solutions? We could expand the neighborhood size to allow two swaps at a time, but this particular bad instance would still stubbornly resist. Instead, let's look at some other generic schemes for improving local search procedures.</p>
  <p> </p>
  <h3 id="dealing-with-local-optima">9.3.3 Dealing with Local Optima</h3>
  <p><strong>Randomization and Restarts</strong></p>
  <p>Randomization can be an invaluable ally in local search. It is typically used in two ways: to pick a random initial solution, for instance a random graph partition; and to choose a local move when several are available.</p>
  <p>When there are many local optima, randomization is a way of making sure that there is at least some probability of getting to the right one. The local search can then be repeated several times, with a different random seed on each invocation, and the best solution returned. If the probability of reaching a good local optimum on any given run is <span class="math inline">\(p\)</span>, then within <span class="math inline">\(O(1 / p)\)</span> runs such a solution is likely to be found (recall Exercise 1.34).</p>
  <p>Figure 9.10 shows a small instance of graph partitioning, along with the search space of solutions. There are a total of <span class="math inline">\(\binom{8}{4} = 70\)</span> possible states, but since each of them has an identical twin in which the left and right sides of the cut are flipped, in effect there are just <span class="math inline">\(35\)</span> solutions.</p>
  <p>In the figure, these are organized into seven groups for readability. There are five local optima, of which four are bad, with cost <span class="math inline">\(2\)</span>, and one is good, with cost <span class="math inline">\(0\)</span>. If local search is started at a random solution, and at each step a random neighbor of lower cost is selected, then the search is at most four times as likely to wind up in a bad solution than a good one. Thus only a small handful of repetitions is needed.</p>
  <div class="figure">
  <img src="fig-9.10-search-space-example.png" alt="Figure 9.10 The search space for a graph with eight nodes. The space contains 35 solutions, which have been partitioned into seven groups for clarity. An example of each is shown. There are five local optima." />
  <p class="caption"><strong>Figure 9.10</strong> The search space for a graph with eight nodes. The space contains <span class="math inline">\(35\)</span> solutions, which have been partitioned into seven groups for clarity. An example of each is shown. There are five local optima.</p>
  </div>
  <p> </p>
  <p><strong>Simulated Annealing</strong></p>
  <p>In the example of Figure 9.10, each run of local search has a reasonable chance of finding the global optimum. This isn't always true. As the problem size grows, the ratio of bad to good local optima often increases, sometimes to the point of being exponentially large. In such cases, simply repeating the local search a few times is ineffective.</p>
  <p>A different avenue of attack is to occasionally allow moves that actually increase the cost, in the hope that they will pull the search out of dead ends. This would be very useful at the bad local optima of Figure 9.10, for instance. The method of <em>simulated annealing</em> redefines the local search by introducing the notion of a <em>temperature</em> <span class="math inline">\(T\)</span>.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">let s be <span class="bu">any</span> starting solution
  repeat
    randomly choose a solution s<span class="op">*</span> <span class="kw">in</span> the neighborhood of s
    <span class="cf">if</span> ∆ <span class="op">=</span> cost(s<span class="op">*</span>) <span class="op">-</span> cost(s) <span class="kw">is</span> negative:
      replace s by s<span class="op">*</span>
    <span class="cf">else</span>:
      replace s by s<span class="op">*</span> <span class="cf">with</span> probability e<span class="op">^</span>(<span class="op">-</span>∆ <span class="op">/</span> T)</code></pre></div>
  <p>If <span class="math inline">\(T\)</span> is zero, this is identical to our previous local search. But if <span class="math inline">\(T\)</span> is large, then moves that increase the cost are occasionally accepted. What value of <span class="math inline">\(T\)</span> should be used?</p>
  <p>The trick is to start with <span class="math inline">\(T\)</span> large and then gradually reduce it to zero. Thus initially, the local search can wander around quite freely, with only a mild preference for low-cost solutions. As time goes on, this preference becomes stronger, and the system mostly sticks to the lower-cost region of the search space, with occasional excursions out of it to escape local optima. Eventually, when the temperature drops further, the system converges on a solution.</p>
  <p>Figure 9.11 shows this process schematically.</p>
  <div class="figure">
  <img src="fig-9.11-simulated-annealing.png" alt="Figure 9.11 Simulated annealing." />
  <p class="caption"><strong>Figure 9.11</strong> Simulated annealing.</p>
  </div>
  <p>Simulated annealing is inspired by the physics of crystallization. When a substance is to be crystallized, it starts in liquid state, with its particles in relatively unconstrained motion. Then it is slowly cooled, and as this happens, the particles gradually move into more regular configurations. This regularity becomes more and more pronounced until finally a crystal lattice is formed.</p>
  <p>The benefits of simulated annealing come at a significant cost: because of the changing temperature and the initial freedom of movement, many more local moves are needed until convergence. Moreover, it is quite an art to choose a good timetable by which to decrease the temperature, called an <em>annealing schedule</em>. But in many cases where the quality of solutions improves significantly, the tradeoff is worthwhile.</p>


  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
