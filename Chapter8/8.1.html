<!DOCTYPE html>
<html>
<head>

  <title>8.1 Search Problems</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="search-problems">8.1 Search Problems</h2>
  <p>Over the past seven chapters we have developed algorithms for finding shortest paths and minimum spanning trees in graphs, matchings in bipartite graphs, maximum increasing subsequences, maximum flows in networks, and so on. All these algorithms are <em>efficient</em>, because in each case their time requirement grows as a polynomial function (such as <span class="math inline">\(n\)</span>, <span class="math inline">\(n^2\)</span>, or <span class="math inline">\(n^3\)</span>) of the size of the input.</p>
  <p>To better appreciate such efficient algorithms, consider the alternative: In all these problems we are searching for a solution (path, tree, matching, etc.) from among an <em>exponential</em> population of possibilities. Indeed, <span class="math inline">\(n\)</span> boys can be matched with <span class="math inline">\(n\)</span> girls in <span class="math inline">\(n!\)</span> different ways, a graph with <span class="math inline">\(n\)</span> vertices has <span class="math inline">\(n^{n-2}\)</span> spanning trees, and a typical graph has an exponential number of paths from <span class="math inline">\(s\)</span> to <span class="math inline">\(t\)</span>.</p>
  <p>All these problems could in principle be solved in exponential time by checking through all candidate solutions, one by one. But an algorithm whose running time is <span class="math inline">\(2^{n}\)</span>, or worse, is all but useless in practice (see the next box). The quest for efficient algorithms is about finding clever ways to bypass this process of exhaustive search, using clues from the input in order to dramatically narrow down the search space.</p>
  <p>So far in this book we have seen the most brilliant successes of this quest, algorithmic techniques that defeat the specter of exponentiality: greedy algorithms, dynamic programming, linear programming (while divide-and-conquer typically yields faster algorithms for problems we can already solve in polynomial time). Now the time has come to meet the quest's most embarrassing and persistent failures.</p>
  <p>We shall see some other &quot;search problems&quot;, in which again we are seeking a solution with particular properties among an exponential chaos of alternatives. But for these new problems no shortcut seems possible. The fastest algorithms we know for them are all exponential—not substantially better than an exhaustive search. We now introduce some important examples.</p>
  <blockquote>
  <p><strong>The Story of Sissa and Moore</strong></p>
  <p>According to the legend, the game of chess was invented by the Brahmin Sissa to amuse and teach his king. Asked by the grateful monarch what he wanted in return, the wise man requested that the king place one grain of rice in the first square of the chessboard, two in the second, four in the third, and so on, doubling the amount of rice up to the 64th square. The king agreed on the spot, and as a result he was the first person to learn the valuable—albeit humbling—lesson of <em>exponential growth</em>. Sissa's request amounted to <span class="math inline">\(2^64 - 1 = 18,446,744,073,709,551,615\)</span> grains of rice, enough rice to pave all of India several times over!</p>
  <p>All over nature, from colonies of bacteria to cells in a fetus, we see systems that grow exponentially—for a while. In 1798, the British philosopher T. Robert Malthus published an essay in which he predicted that the exponential growth (he called it &quot;geometric growth&quot;) of the human population would soon deplete linearly growing resources, an argument that influenced Charles Darwin deeply. Malthus knew the fundamental fact that an exponential sooner or later takes over any polynomial.</p>
  <p>In 1965, computer chip pioneer Gordon E. Moore noticed that transistor density in chips had doubled every year in the early 1960s, and he predicted that this trend would continue. This prediction, moderated to a doubling every 18 months and extended to computer speed, is known as <em>Moore's law</em>. It has held remarkably well for 40 years. And these are the two root causes of the explosion of information technology in the past decades: <em>Moore's law and efficient algorithms</em>.</p>
  <p>It would appear that Moore's law provides a disincentive for developing polynomial algorithms. After all, if an algorithm is exponential, why not wait it out until Moore's law makes it feasible? But in reality the exact opposite happens: Moore's law is a huge incentive for developing efficient algorithms, because such algorithms are needed in order to take advantage of the exponential increase in computer speed.</p>
  <p>Here is why. If, for example, an <span class="math inline">\(O(2^n)\)</span> algorithm for Boolean satisfiability (<span class="math inline">\(\text{SAT}\)</span>) were given an hour to run, it would have solved instances with 25 variables back in 1975, 31 variables on the faster computers available in 1985, 38 variables in 1995, and about 45 variables with today's machines. Quite a bit of progress—except that each extra variable requires a year and a half's wait, while the appetite of applications (many of which are, ironically, related to computer design) grows much faster.</p>
  <p>In contrast, the size of the instances solved by an <span class="math inline">\(O(n)\)</span> or <span class="math inline">\(O(n\log{n})\)</span> algorithm would be multiplied by a factor of about 100 each decade. In the case of an <span class="math inline">\(O(n^2)\)</span> algorithm, the instance size solvable in a fixed time would be multiplied by about 10 each decade. Even an <span class="math inline">\(O(n^6)\)</span> algorithm, polynomial yet unappetizing, would more than double the size of the instances solved each decade. When it comes to the growth of the size of problems we can attack with an algorithm, we have a reversal: <em>exponential algorithms make polynomially slow progress, while polynomial algorithms advance exponentially fast!</em> For Moore's law to be reflected in the world we <em>need</em> efficient algorithms.</p>
  <p>As Sissa and Malthus knew very well, exponential expansion cannot be sustained in- definitely in our finite world. Bacterial colonies run out of food; chips hit the atomic scale. Moore's law will stop doubling the speed of our computers within a decade or two. And then progress will depend on algorithmic ingenuity—or otherwise perhaps on novel ideas such as <em>quantum computation</em>, explored in Chapter 10.</p>
  </blockquote>
  <h3 id="satisfiability">Satisfiability</h3>
  <p><span class="math inline">\(\text{SATISFIABILITY}\)</span>, or <span class="math inline">\(\text{SAT}\)</span> (recall Exercise 3.28 and Section 5.3), is a problem of great practical importance, with applications ranging from chip testing and computer design to image analysis and software engineering. It is also a canonical hard problem. Here's what an instance of <span class="math inline">\(\text{SAT}\)</span> looks like:</p>
  <p><span class="math display">\[(x \vee y \vee z)(x \vee \hat{y})(y \vee \hat{z})(z \vee \hat{x})(\hat{x} \vee \hat{y} \vee \hat{z}).\]</span></p>
  <p>This is a <em>Boolean formula</em> in <strong>conjunctive normal form (CNF)</strong>. It is a collection of clauses (the parentheses), each consisting of the disjunction (logical <em>or</em>, denoted <span class="math inline">\(\vee\)</span>) of several literals, where a literal is either a Boolean variable (such as <span class="math inline">\(x\)</span>) or the negation of one (such as <span class="math inline">\(\hat{x}\)</span>). A satisfying truth assignment is an assignment of false or true to each variable so that every clause contains a literal whose value is true. The <span class="math inline">\(\text{SAT}\)</span> problem is the following: given a Boolean formula in conjunctive normal form, either find a satisfying truth assignment or else report that none exists.</p>
  <p>In the instance shown previously, setting all variables to <span class="math inline">\(\texttt{true}\)</span>, for example, satisfies every clause except the last. Is there a truth assignment that satisfies <em>all</em> clauses?</p>
  <p>With a little thought, it is not hard to argue that in this particular case no such truth assignment exists. (Hint: The three middle clauses constrain all three variables to have the same value.) But how do we decide this in general? Of course, we can always search through all truth assignments, one by one, but for formulas with <span class="math inline">\(n\)</span> variables, the number of possible assignments is exponential, <span class="math inline">\(2^n\)</span>.</p>
  <p><span class="math inline">\(\text{SAT}\)</span> is a typical <em>search problem</em>. We are given an instance <span class="math inline">\(I\)</span> (that is, some input data specifying the problem at hand, in this case a Boolean formula in conjunctive normal form), and we are asked to find a <em>solution</em> <span class="math inline">\(S\)</span> (an object that meets a particular specification, in this case an assignment that satisfies each clause). If no such solution exists, we must say so.</p>
  <p>More specifically, a search problem must have the property that any proposed solution <span class="math inline">\(S\)</span> to an instance <span class="math inline">\(I\)</span> can be quickly checked for correctness. What does this entail? For one thing, <span class="math inline">\(S\)</span> must at least be concise (quick to read), with length polynomially bounded by that of <span class="math inline">\(I\)</span>. This is clearly true in the case of <span class="math inline">\(\text{SAT}\)</span>, for which <span class="math inline">\(S\)</span> is an assignment to the variables. To formalize the notion of quick checking, we will say that there is a polynomial-time algorithm that takes as input <span class="math inline">\(I\)</span> and <span class="math inline">\(S\)</span> and decides whether or not <span class="math inline">\(S\)</span> is a solution of <span class="math inline">\(I\)</span>. For <span class="math inline">\(\text{SAT}\)</span>, this is easy as it just involves checking whether the assignment specified by <span class="math inline">\(S\)</span> indeed satisfies every clause in <span class="math inline">\(I\)</span>.</p>
  <p>Later in this chapter it will be useful to shift our vantage point and to think of this efficient algorithm for checking proposed solutions as <em>defining</em> the search problem. Thus:</p>
  <ul>
  <li>A search problem is specified by an algorithm <span class="math inline">\(C\)</span> that takes two inputs, an instance <span class="math inline">\(I\)</span> and a proposed solution <span class="math inline">\(S\)</span>, and runs in time polynomial in <span class="math inline">\(|I|\)</span>. We say <span class="math inline">\(S\)</span> is a solution to <span class="math inline">\(I\)</span> if and only if <span class="math inline">\(C(I, S) = \texttt{true}\)</span>.</li>
  </ul>
  <p>Given the importance of the <span class="math inline">\(\text{SAT}\)</span> search problem, researchers over the past 50 years have tried hard to find efficient ways to solve it, but without success. The fastest algorithms we have are still exponential on their worst-case inputs.</p>
  <p>Yet, interestingly, there are two natural variants of <span class="math inline">\(\text{SAT}\)</span> for which we do have good algorithms. If all clauses contain at most one positive literal, then the Boolean formula is called a <em>Horn formula</em>, and a satisfying truth assignment, if one exists, can be found by the greedy algorithm of Section 5.3. Alternatively, if all clauses have only two literals, then graph theory comes into play, and <span class="math inline">\(\text{SAT}\)</span> can be solved in linear time by finding the strongly connected components of a particular graph constructed from the instance (recall Exercise 3.28). In fact, in Chapter 9, we'll see a different polynomial algorithm for this same special case, which is called <span class="math inline">\(\text{2SAT}\)</span> .</p>
  <p>On the other hand, if we are just a little more permissive and allow clauses to contain <em>three</em> literals, then the resulting problem, known as <span class="math inline">\(\text{3SAT}\)</span> (an example of which we saw earlier), once again becomes hard to solve!</p>
  <h3 id="the-traveling-salesman-problem">The Traveling Salesman Problem</h3>
  <p>In the traveling salesman problem (<span class="math inline">\(\text{TSP}\)</span>) we are given <span class="math inline">\(n\)</span> vertices <span class="math inline">\(1, \cdots, n\)</span> and all <span class="math inline">\(n(n - 1)/2\)</span> distances between them, as well as a <em>budget</em> <span class="math inline">\(b\)</span>. We are asked to find a tour, a cycle that passes through every vertex exactly once, of total cost <span class="math inline">\(b\)</span> or less—or to report that no such tour exists. That is, we seek a permutation <span class="math inline">\(\tau(1), \cdots, \tau(n)\)</span> of the vertices such that when they are toured in this order, the total distance covered is at most <span class="math inline">\(b\)</span>:</p>
  <p><span class="math display">\[d_{\tau(1), \tau(2)} + d_{\tau(2), \tau(3)} + \cdots + d_{\tau(n), \tau(1)} \leq b.\]</span></p>
  <p>See Figure 8.1 for an example (only some of the distances are shown; assume the rest are very large).</p>
  <div class="figure">
  <img src="fig-8.1-tsp-example.png" alt="Figure 8.1 The optimal traveling salesman tour, shown in bold, has length 18." />
  <p class="caption"><strong>Figure 8.1</strong> The optimal traveling salesman tour, shown in bold, has length 18.</p>
  </div>
  <p>Notice how we have defined the <span class="math inline">\(\text{TSP}\)</span> as a <em>search problem</em>: given an instance, find a tour within the budget (or report that none exists). But why are we expressing the traveling salesman problem in this way, when in reality it is an <em>optimization problem</em>, in which the <em>shortest</em> possible tour is sought? Why dress it up as something else?</p>
  <p>For a good reason. Our plan in this chapter is to compare and relate problems. The framework of search problems is helpful in this regard, because it encompasses optimization problems like the <span class="math inline">\(\text{TSP}\)</span> in addition to true search problems like SAT.</p>
  <p>Turning an optimization problem into a search problem does not change its difficulty at all, because the two versions <em>reduce to one another</em>. Any algorithm that solves the optimization <span class="math inline">\(\text{TSP}\)</span> also readily solves the search problem: find the optimum tour and if it is within budget, return it; if not, there is no solution.</p>
  <p>Conversely, an algorithm for the search problem can also be used to solve the optimization problem. To see why, first suppose that we somehow knew the cost of the optimum tour; then we could find this tour by calling the algorithm for the search problem, using the optimum cost as the budget. Fine, but how do we find the optimum cost? Easy: By binary search! (See Exercise 8.1.)</p>
  <p>Incidentally, there is a subtlety here: <em>Why do we have to introduce a budget?</em> Isn't any optimization problem also a search problem in the sense that we are searching for a solution that has the property of being optimal? The catch is that the solution to a search problem should be easy to recognize, or as we put it earlier, polynomial-time checkable. Given a potential solution to the <span class="math inline">\(\text{TSP}\)</span>, it is easy to check the properties &quot;is a tour&quot; (just check that each vertex is visited exactly once) and &quot;has total length <span class="math inline">\(\leq b\)</span>.&quot; But how could one check the property &quot;is optimal&quot;?</p>
  <p>As with SAT, there are no known polynomial-time algorithms for the <span class="math inline">\(\text{TSP}\)</span>, despite much effort by researchers over nearly a century. Of course, there is an exponential algorithm for solving it, by trying all <span class="math inline">\((n - 1)!\)</span> tours, and in Section 6.6 we saw a faster, yet still exponential, dynamic programming algorithm.</p>
  <p>The minimum spanning tree (MST) problem, for which we <em>do</em> have efficient algorithms, provides a stark contrast here. To phrase it as a search problem, we are again given a distance matrix and a bound <span class="math inline">\(b\)</span>, and are asked to find a tree <span class="math inline">\(T\)</span> with total weight <span class="math inline">\((i,j) \in T,\ d_{ij} \leq b\)</span>. The <span class="math inline">\(\text{TSP}\)</span> can be thought of as a tough cousin of the MST problem, in which the tree is not allowed to branch and is therefore a path.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a> This extra restriction on the structure of the tree results in a much harder problem.</p>
  <h3 id="euler-and-rudrata">Euler and Rudrata</h2>
  <p>In the summer of 1735 Leonhard Euler (pronounced &quot;Oiler&quot;), the famous Swiss mathematician, was walking the bridges of the East Prussian town of Konigsberg. After a while, he noticed in frustration that, no matter where he started his walk, no matter how cleverly he continued, it was impossible to cross each bridge exactly once. And from this silly ambition, the field of graph theory was born.</p>
  <p>Euler identified at once the roots of the park's deficiency. First, you turn the map of the park into a graph whose vertices are the four land masses (two islands, two banks) and whose edges are the seven bridges:</p>
  <div class="figure">
  <img src="seven-bridges.png" />

  </div>
  <p>This graph has multiple edges between two vertices—a feature we have not been allowing so far in this book, but one that is meaningful for this particular problem, since each bridge must be accounted for separately. We are looking for a path that goes through each edge exactly once (the path is allowed to repeat vertices). In other words, we are asking this question: <em>When can a graph be drawn without lifting the pencil from the paper?</em></p>
  <p>The answer discovered by Euler is simple, elegant, and intuitive: If and only if (a) the graph is connected and (b) every vertex, with the possible exception of two vertices (the start and final vertices of the walk), has even degree (Exercise 3.26). This is why Konigsberg's park was impossible to traverse: all four vertices have odd degree.</p>
  <p>To put it in terms of our present concerns, let us define a search problem called <span class="math inline">\(\text{EULER PATH}\)</span>: given a graph, find a path that contains each edge exactly once. It follows from Euler's observation, and a little more thinking, that this search problem can be solved in polynomial time.</p>
  <p>Almost a millennium before Euler's fateful summer in East Prussia, a Kashmiri poet named Rudrata had asked this question: <em>Can one visit all the squares of the chessboard, without repeating any square, in one long walk that ends at the starting square and at each step makes a legal knight move?</em> This is again a graph problem: the graph now has 64 vertices, and two squares are joined by an edge if a knight can go from one to the other in a single move (that is, if their coordinates differ by 2 in one dimension and by 1 in the other). See Figure 8.2 for the portion of the graph corresponding to the upper left corner of the board. Can you find a knight's tour on your chessboard?</p>
  <div class="figure">
  <img src="fig-8.2-rudrata-knights.png" alt="Figure 8.2 Knight's moves on a corner of a chessboard." />
  <p class="caption"><strong>Figure 8.2</strong> Knight's moves on a corner of a chessboard.</p>
  </div>
  <p>This is a different kind of search problem in graphs: we want a cycle that goes through all vertices (as opposed to all edges in Euler's problem), without repeating any vertex. And there is no reason to stick to chessboards; this question can be asked of any graph. Let us define the <span class="math inline">\(\text{RUDRATA CYCLE}\)</span> search problem to be the following: given a graph, find a cycle that visits each vertex exactly once—or report that no such cycle exists.<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a> This problem is ominously reminiscent of the <span class="math inline">\(\text{TSP}\)</span>, and indeed no polynomial algorithm is known for it.</p>
  <p>There are two differences between the definitions of the Euler and Rudrata problems. The first is that Euler's problem visits all <em>edges</em> while Rudrata's visits all <em>vertices</em>. But there is also the issue that one of them demands a path while the other requires a cycle. Which of these differences accounts for the huge disparity in computational complexity between the two problems? It must be the first, because the second difference can be shown to be purely cosmetic. Indeed, define the <span class="math inline">\(\text{RUDRATA CYCLE}\)</span> problem to be just like <span class="math inline">\(\text{RUDRATA CYCLE}\)</span>, except that the goal is now to find a <em>path</em> that goes through each vertex exactly once. As we will soon see, there is a precise equivalence between the two versions of the Rudrata problem.</p>
  <h3 id="cuts-and-bisections">Cuts and Bisections</h3>
  <p>A <strong>cut</strong> is a set of edges whose removal leaves a graph disconnected. It is often of interest to find small cuts, and the <span class="math inline">\(\text{MINIMUM CUT}\)</span> problem is, given a graph and a budget <span class="math inline">\(b\)</span>, to find a cut with at most <span class="math inline">\(b\)</span> edges. For example, the smallest cut in Figure 8.3 is of size 3. This problem can be solved in polynomial time by <span class="math inline">\(n - 1\)</span> max-flow computations: give each edge a capacity of 1, and find the maximum flow between some fixed node and every single other node. The smallest such flow will correspond (via the max-flow min-cut theorem) to the smallest cut. Can you see why? We've also seen a very different, randomized algorithm for this problem (page 143).</p>
  <div class="figure">
  <img src="fig-8.3-min-cut-example.png" alt="Figure 8.3 What is the smallest cut in this graph?" />
  <p class="caption"><strong>Figure 8.3</strong> What is the smallest cut in this graph?</p>
  </div>
  <p>In many graphs, such as the one in Figure 8.3, the smallest cut leaves just a singleton vertex on one side—it consists of all edges adjacent to this vertex. Far more interesting are small cuts that partition the vertices of the graph into nearly equal-sized sets. More precisely, the <span class="math inline">\(\text{BALANCED CUT}\)</span> problem is this: given a graph with <span class="math inline">\(n\)</span> vertices and a budget <span class="math inline">\(b\)</span>, partition the vertices into two sets <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span> such that <span class="math inline">\(|S|,|T| \geq n/3\)</span> and such that there are at most <span class="math inline">\(b\)</span> edges between <span class="math inline">\(S\)</span> and <span class="math inline">\(T\)</span>. Another hard problem.</p>
  <p>Balanced cuts arise in a variety of important applications, such as <em>clustering</em>. Consider for example the problem of segmenting an image into its constituent components (say, an elephant standing in a grassy plain with a clear blue sky above). A good way of doing this is to create a graph with a node for each pixel of the image and to put an edge between nodes whose corresponding pixels are spatially close together and are also similar in color. A single object in the image (like the elephant, say) then corresponds to a set of highly connected vertices in the graph. A balanced cut is therefore likely to divide the pixels into two clusters without breaking apart any of the primary constituents of the image. The first cut might, for instance, separate the elephant on the one hand from the sky and from grass on the other. A further cut would then be needed to separate the sky from the grass.</p>
  <h3 id="integer-linear-programming">Integer Linear Programming</h3>
  <p>Even though the simplex algorithm is not polynomial time, we mentioned in Chapter 7 that there is a different, polynomial algorithm for linear programming. Therefore, linear programming is efficiently solvable both in practice and in theory. But the situation changes completely if, in addition to specifying a linear objective function and linear inequalities, we also constrain the solution (the values for the variables) to be <em>integer</em>. This latter problem is called <span class="math inline">\(\text{INTEGER LINEAR PROGRAMMING (ILP)}\)</span>.</p>
  <p>Let's see how we might formulate it as a search problem. We are given a set of linear inequalities <span class="math inline">\(\textbf{A}x \leq b\)</span>, where <span class="math inline">\(\textbf{A}\)</span> is an <span class="math inline">\(m \times n\)</span> matrix and <span class="math inline">\(b\)</span> is an <span class="math inline">\(m\)</span>-vector; an objective function specified by an <span class="math inline">\(n\)</span>-vector <span class="math inline">\(c\)</span>; and finally, a goal <span class="math inline">\(g\)</span> (the counterpart of a budget in maximization problems). We want to find a nonnegative integer <span class="math inline">\(n\)</span>-vector <span class="math inline">\(x\)</span> such that <span class="math inline">\(\textbf{A}x \leq b\)</span> and <span class="math inline">\(c \cdot x \geq g\)</span>.</p>
  <p>But there is a redundancy here: the last constraint <span class="math inline">\(c \cdot x \geq g\)</span> is itself a linear inequality and can be absorbed into <span class="math inline">\(\textbf{A}x \leq b\)</span>. So, we define <span class="math inline">\(\text{ILP}\)</span> to be following search problem: given <span class="math inline">\(A\)</span> and <span class="math inline">\(b\)</span>, find a nonnegative integer vector <span class="math inline">\(x\)</span> satisfying the inequalities <span class="math inline">\(\textbf{A}x \leq b\)</span>, or report that none exists. Despite the many crucial applications of this problem, and intense interest by researchers, no efficient algorithm is known for it.</p>
  <p>There is a particularly clean special case of <span class="math inline">\(\text{ILP}\)</span> that is very hard in and of itself: the goal is to find a vector <span class="math inline">\(x\)</span> of <span class="math inline">\(0\)</span>'s and <span class="math inline">\(1\)</span>'s satisfying <span class="math inline">\(\textbf{A}x = 1\)</span>, where <span class="math inline">\(A\)</span> is an <span class="math inline">\(m \times n\)</span> matrix with <span class="math inline">\(0-1\)</span> entries and <span class="math inline">\(1\)</span> is the <span class="math inline">\(m\)</span>-vector of all <span class="math inline">\(1\)</span>'s. It should be apparent from the reductions in Section 7.1.4 that this is indeed a special case of <span class="math inline">\(\text{ILP}\)</span>. We call it <span class="math inline">\(\text{ZERO-ONE EQUATIONS (ZOE)}\)</span>.</p>
  <p>We have now introduced a number of important search problems, some of which are familiar from earlier chapters and for which there are efficient algorithms, and others which are different in small but crucial ways that make them very hard computational problems. To complete our story we will introduce a few more hard problems, which will play a role later in the chapter, when we relate the computational difficulty of all these problems. The reader is invited to skip ahead to Section 8.2 and then return to the definitions of these problems as required.</p>
  <h3 id="three-dimensional-matching">Three-Dimensional Matching</h3>
  <p>Recall the <span class="math inline">\(\text{BIPARTITE MATCHING}\)</span> problem: given a bipartite graph with <span class="math inline">\(n\)</span> nodes on each side (the boys and the girls), find a set of <span class="math inline">\(n\)</span> disjoint edges, or decide that no such set exists. In Section 7.3, we saw how to efficiently solve this problem by a reduction to maximum flow.</p>
  <p>However, there is an interesting generalization, called <span class="math inline">\(\text{3D MATCHING}\)</span>, for which no polynomial algorithm is known. In this new setting, there are <span class="math inline">\(n\)</span> boys and <span class="math inline">\(n\)</span> girls, but also <span class="math inline">\(n\)</span> pets, and the compatibilities among them are specified by a set of triples, each containing a boy, a girl, and a pet. Intuitively, a triple (<span class="math inline">\(b, g, p\)</span>) means that boy <span class="math inline">\(b\)</span>, girl <span class="math inline">\(g\)</span>, and pet <span class="math inline">\(p\)</span> get along well together. We want to find <span class="math inline">\(n\)</span> disjoint triples and thereby create <span class="math inline">\(n\)</span> harmonious households. Can you spot a solution in Figure 8.4?</p>
  <div class="figure">
  <img src="fig-8.4-3d-matching-example.png" alt="Figure 8.4 A more elaborate matchmaking scenario. Each triple is shown as a triangular- shaped node joining boy, girl, and pet." />
  <p class="caption"><strong>Figure 8.4</strong> A more elaborate matchmaking scenario. Each triple is shown as a triangular- shaped node joining boy, girl, and pet.</p>
  </div>
  <h3 id="independent-set-vertex-cover-and-clique">Independent Set, Vertex Cover, and Clique</h3>
  <p>In the <span class="math inline">\(\text{INDEPENDENT SET}\)</span> problem (recall Section 6.7) we are given a graph and an integer <span class="math inline">\(g\)</span>, and the aim is to find <span class="math inline">\(g\)</span> vertices that are independent, that is, no two of which have an edge between them. Can you find an independent set of three vertices in Figure 8.5? How about four vertices? We saw in Section 6.7 that this problem can be solved efficiently on trees, but for general graphs no polynomial algorithm is known.</p>
  <div class="figure">
  <img src="fig-8.5-independent-set-example.png" alt="Figure 8.5 What is the size of the largest independent set in this graph?" />
  <p class="caption"><strong>Figure 8.5</strong> What is the size of the largest independent set in this graph?</p>
  </div>
  <p>There are many other search problems about graphs. In <span class="math inline">\(\text{VERTEX COVER}\)</span>, for example, the input is a graph and a budget <span class="math inline">\(b\)</span>, and the idea is to find <span class="math inline">\(b\)</span> vertices that cover (touch) every edge. Can you cover all edges of Figure 8.5 with seven vertices? With six? (And do you see the intimate connection to the <span class="math inline">\(\text{INDEPENDENT SET}\)</span> problem?)</p>
  <p><span class="math inline">\(\text{VERTEX COVER}\)</span> is a special case of <span class="math inline">\(\text{SET COVER}\)</span>, which we encountered in Chapter 5. In that problem, we are given a set <span class="math inline">\(E\)</span> and several subsets of it, <span class="math inline">\(S_1, \cdots, S_m\)</span>, along with a budget <span class="math inline">\(b\)</span>. We are asked to select <span class="math inline">\(b\)</span> of these subsets so that their union is <span class="math inline">\(E\)</span>. <span class="math inline">\(\text{VERTEX COVER}\)</span> is the special case in which <span class="math inline">\(E\)</span> consists of the edges of a graph, and there is a subset <span class="math inline">\(S_i\)</span> for each vertex, containing the edges adjacent to that vertex. Can you see why <span class="math inline">\(\text{3D MATCHING}\)</span> is also a special case of <span class="math inline">\(\text{SET COVER}\)</span>?</p>
  <p>And finally there is the <span class="math inline">\(\text{CLIQUE}\)</span> problem: given a graph and a goal <span class="math inline">\(g\)</span>, find a set of <span class="math inline">\(g\)</span> vertices such that all possible edges between them are present. What is the largest clique in Figure 8.5?</p>
  <h3 id="longest-path">Longest Path</h3>
  <p>We know the shortest-path problem can be solved very efficiently, but how about the <span class="math inline">\(\text{LONGEST PATH}\)</span> problem? Here we are given a graph <span class="math inline">\(G\)</span> with nonnegative edge weights and two distinguished vertices <span class="math inline">\(s\)</span> and <span class="math inline">\(t\)</span>, along with a goal <span class="math inline">\(g\)</span>. We are asked to find a path from <span class="math inline">\(s\)</span> to <span class="math inline">\(t\)</span> with total weight at least <span class="math inline">\(g\)</span>. Naturally, to avoid trivial solutions we require that the path be <em>simple</em>, containing no repeated vertices.</p>
  <p>No efficient algorithm is known for this problem (which sometimes also goes by the name of <span class="math inline">\(\text{TAXICAB RIP-OFF}\)</span>).</p>
  <h3 id="knapsack-and-subset-sum">Knapsack and Subset Sum</h3>
  <p>Recall the <span class="math inline">\(\text{KNAPSACK}\)</span> problem (Section 6.4): we are given integer weights <span class="math inline">\(w_1, \cdots, w_n\)</span> and integer values <span class="math inline">\(v_1, \cdots, v_n\)</span> for <span class="math inline">\(n\)</span> items. We are also given a weight capacity <span class="math inline">\(W\)</span> and a goal <span class="math inline">\(g\)</span> (the former is present in the original optimization problem, the latter is added to make it a search problem). We seek a set of items whose total weight is at most <span class="math inline">\(W\)</span> and whose total value is at least <span class="math inline">\(g\)</span>. As always, if no such set exists, we should say so.</p>
  <p>In Section 6.4, we developed a dynamic programming scheme for <span class="math inline">\(\text{KNAPSACK}\)</span> with running time <span class="math inline">\(O(nW)\)</span>, which we noted is exponential in the input size, since it involves <span class="math inline">\(W\)</span> rather than <span class="math inline">\(\log{W}\)</span>. And we have the usual exhaustive algorithm as well, which looks at all subsets of items—all <span class="math inline">\(2^n\)</span> of them. Is there a polynomial algorithm for <span class="math inline">\(\text{KNAPSACK}\)</span>? Nobody knows of one.</p>
  <p>But suppose that we are interested in the variant of the knapsack problem in which the integers are coded in <em>unary</em>—for instance, by writing <span class="math inline">\(111111111111\)</span> for <span class="math inline">\(12\)</span>. This is admittedly an exponentially wasteful way to represent integers, but it does define a legitimate problem, which we could call <span class="math inline">\(\text{UNARY KNAPSACK}\)</span>. It follows from our discussion that this somewhat artificial problem does have a polynomial algorithm.</p>
  <p>A different variation: suppose now that each item's value is equal to its weight (all given in binary), and to top it off, the goal <span class="math inline">\(g\)</span> is the same as the capacity <span class="math inline">\(W\)</span>. (To adapt the silly break-in story whereby we first introduced the knapsack problem, the items are all gold nuggets, and the burglar wants to fill his knapsack to the hilt.) This special case is tantamount to finding a subset of a given set of integers that adds up to exactly <span class="math inline">\(W\)</span>. Since it is a special case of <span class="math inline">\(\text{KNAPSACK}\)</span>, it cannot be any harder. But could it be polynomial? As it turns out, this problem, called <span class="math inline">\(\text{SUBSET SUM}\)</span>, is also very hard.</p>
  <p>At this point one could ask: If <span class="math inline">\(\text{SUBSET SUM}\)</span> is a special case that happens to be as hard as the general <span class="math inline">\(\text{KNAPSACK}\)</span> problem, why are we interested in it? The reason is <em>simplicity</em>. In the complicated calculus of reductions between search problems that we shall develop in this chapter, conceptually simple problems like <span class="math inline">\(\text{SUBSET SUM}\)</span> and <span class="math inline">\(\text{3SAT}\)</span> are invaluable.</p>
  <div class="footnotes">
  <hr />
  <ol>
  <li id="fn1"><p>Actually the <span class="math inline">\(\text{TSP}\)</span> demands a cycle, but one can define an alternative version that seeks a path, and it is not hard to see that this is just as hard as the <span class="math inline">\(\text{TSP}\)</span> itself.<a href="#fnref1">↩</a></p></li>
  <li id="fn2"><p>In the literature this problem is known as the <em>Hamilton cycle</em> problem, after the great Irish mathematician who rediscovered it in the 19th century.<a href="#fnref2">↩</a></p></li>
  </ol>
  </div>

  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
