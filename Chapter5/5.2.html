<!DOCTYPE html>
<html>
<head>

  <title>5.2 Huffman encoding</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="huffman-encoding">5.2 Huffman Encoding</h2>
  <p>In the MP3 audio compression scheme, a sound signal is encoded in three steps.</p>
  <ol style="list-style-type: decimal">
  <li><p>It is digitized by sampling at regular intervals, yielding a sequence of real numbers <span class="math inline">\(s_1, s_2, \ldots, s_T\)</span>. For instance, at a rate of <span class="math inline">\(44,100\)</span> samples per second, a <span class="math inline">\(50\)</span>-minute symphony would correspond to <span class="math inline">\(T = 50 \times 60 \times 44,100 \approx 130\)</span> million measurements.<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></p></li>
  <li><p>Each real-valued sample <span class="math inline">\(s_t\)</span> is <em>quantized</em>: approximated by a nearby number from a finite set <span class="math inline">\(\Gamma\)</span>. This set is carefully chosen to exploit human perceptual limitations, with the intention that the approximating sequence is indistinguishable from <span class="math inline">\(s_1, s_2, \ldots, s_T\)</span> by the human ear.</p></li>
  <li><p>The resulting string of length <span class="math inline">\(T\)</span> over alphabet <span class="math inline">\(\Gamma\)</span> is encoded in binary.</p></li>
  </ol>
  <p>It is in the last step that Huffman encoding is used. To understand its role, let’s look at a toy example in which <span class="math inline">\(T\)</span> is <span class="math inline">\(130\)</span> million and the alphabet <span class="math inline">\(\Gamma\)</span> consists of just four values, denoted by the symbols <span class="math inline">\(A, B, C, D\)</span>. What is the most economical way to write this long string in binary? The obvious choice is to use <span class="math inline">\(2\)</span> bits per symbol—say codeword <span class="math inline">\(00\)</span> for <span class="math inline">\(A\)</span>, <span class="math inline">\(01\)</span> for <span class="math inline">\(B\)</span>, <span class="math inline">\(10\)</span> for <span class="math inline">\(C\)</span>, and <span class="math inline">\(11\)</span> for <span class="math inline">\(D\)</span>—in which case <span class="math inline">\(260\)</span> megabits are needed in total. Can there possibly be a better encoding than this?</p>
  <p>In search of inspiration, we take a closer look at our particular sequence and find that the four symbols are not equally abundant. <span class="math display">\[\begin{array}{c r} \text{symbol} &amp; \text{frequency} \\ \begin{matrix} A \\ B \\ C \\ D \end{matrix} &amp; \begin{matrix} \text{$70$ million} \\ \text{$3$ million} \\\text{$20$ million} \\ \text{$37$ million} \end{matrix} \end{array}\]</span></p>
  <p>Is there some sort of <em>variable-length encoding</em>, in which just one bit is used for the frequently occurring symbol A, possibly at the expense of needing three or more bits for less common symbols?</p>
  <p>A danger with having codewords of different lengths is that the resulting encoding may not be uniquely decipherable. For instance, if the codewords are <span class="math inline">\(\{0, 01, 11, 001\}\)</span>, the decoding of strings like <span class="math inline">\(001\)</span> is ambiguous. We will avoid this problem by insisting on the <em>prefix-free property</em>: no codeword can be a prefix of another codeword.</p>
  <p>Any prefix-free encoding can be represented by a <em>full</em> binary tree—that is, a binary tree in which every node has either zero or two children—where the symbols are at the leaves, and where each codeword is generated by a path from root to leaf, interpreting left as <span class="math inline">\(0\)</span> and right as <span class="math inline">\(1\)</span> (Exercise 5.28).</p>
  <p>Figure 5.10 shows an example of such an encoding for the four symbols <span class="math inline">\(A, B, C, D\)</span>. Decoding is unique: a string of bits is decrypted by starting at the root, reading the string from left to right to move downward, and, whenever a leaf is reached, outputting the corresponding symbol and returning to the root. It is a simple scheme and pays off nicely for our toy example, where (under the codes of Figure 5.10) the total size of the binary string drops to <span class="math inline">\(213\)</span> megabits, a <span class="math inline">\(17\%\)</span> improvement.</p>
  <p>In general, how do we find the optimal coding tree, given the frequencies <span class="math inline">\(f_1, f_2, \ldots, f_n\)</span> of <span class="math inline">\(n\)</span> symbols? To make the problem precise, we want a tree whose leaves each correspond to a symbol and which minimizes the overall length of the encoding (the number of bits required for a symbol is exactly its depth in the tree): <span class="math display">\[\text{cost of tree} = \sum_{i = 1}^{n} f_i \cdot (\text{depth of $i$th symbol in tree})\]</span></p>
  <p>There is another way to write this cost function that is very helpful. Although we are only given frequencies for the leaves, we can define the frequency of any <em>internal</em> node to be the sum of the frequencies of its descendant leaves; this is, after all, the number of times the internal node is visited during encoding or decoding. During the encoding process, each time we move down the tree, one bit gets output for every non-root node through which we pass. So the total cost—the total number of bits which are output—can also be expressed thus:</p>
  <ul>
  <li>The cost of a tree is the sum of the frequencies of all leaves and internal nodes, except the root.</li>
  </ul>
  <p>The first formulation of the cost function tells us that the <em>two symbols with the smallest frequencies must be at the bottom of the optimal tree</em>, as children of the lowest internal node (this internal node has two children since the tree is <em>full</em>). Otherwise, swapping these two symbols with whatever is lowest in the tree would improve the encoding.</p>
  <p>This suggests that we start constructing the tree <em>greedily</em>: find the two symbols with the smallest frequencies, say <span class="math inline">\(i\)</span> and <span class="math inline">\(j\)</span>, and make them children of a new node, which then has frequency <span class="math inline">\(f_i + f_j\)</span>. To keep the notation simple, let’s just assume these are <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span>. By the second formulation of the cost function, any tree in which <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> are sibling-leaves has cost <span class="math inline">\(f_1 + f_2\)</span> plus the cost for a tree with <span class="math inline">\(n - 1\)</span> leaves of frequencies <span class="math inline">\((f_1 + f_2), f_3, f_4, \ldots, f_n\)</span>: <img src="huffman-tree-greedy.png" /></p>
  <p>The latter problem is just a smaller version of the one we started with. So we pull <span class="math inline">\(f_1\)</span> and <span class="math inline">\(f_2\)</span> off the list of frequencies, insert <span class="math inline">\((f_1 + f_2)\)</span>, and loop. The resulting algorithm can be described in terms of priority queue operations (as defined on page 114) and takes <span class="math inline">\(O(n \log{n})\)</span> time if a binary heap (Section 4.5.2) is used.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python"><span class="kw">def</span> Huffman(f):
    <span class="co">&quot;&quot;&quot;</span>
  <span class="co">  Input: an array f [1 ··· n] of frequencies</span>
  <span class="co">  Output: an encoding tree with n leaves</span>
  <span class="co">  &quot;&quot;&quot;</span>

    let H be a priority queue of integers ordered by f

    <span class="cf">for</span> i <span class="op">=</span> <span class="dv">1</span> to n:
      insert(H, i)

    <span class="cf">for</span> k <span class="op">=</span> n <span class="op">+</span> <span class="dv">1</span> to 2n - <span class="dv">1</span>:
      i <span class="op">=</span> deletemin(H), j <span class="op">=</span> deletemin(H)
      create a node numbered k <span class="cf">with</span> children i, j
      f[k] <span class="op">=</span> f[i] <span class="op">+</span> f[j]
      insert(H, k)

    <span class="cf">return</span> H</code></pre></div>
  <p>Returning to our toy example: can you tell if the tree of Figure 5.10 is optimal?</p>
  <p> </p>
  <blockquote>
  <p><strong>Entropy</strong></p>
  <p>The annual county horse race is bringing in three thoroughbreds who have never competed against one another. Excited, you study their past <span class="math inline">\(200\)</span> races and summarize these as probability distributions over four outcomes: <span class="math inline">\(\texttt{first}\)</span> (“first place”), <span class="math inline">\(\texttt{second, third}\)</span>, and <span class="math inline">\(\texttt{other}\)</span>. <span class="math display">\[\begin{array}{c c c c} \text{outcome} &amp; \text{Aurora} &amp; \text{Whirlwind} &amp; \text{Phantasm} \\ \begin{matrix} \texttt{first} \\ \texttt{second} \\ \texttt{third} \\ \texttt{other} \end{matrix} &amp; \begin{matrix} 0.15 \\ 0.10 \\ 0.70 \\ 0.05 \end{matrix} &amp; \begin{matrix} 0.30 \\ 0.05 \\ 0.25 \\ 0.40 \end{matrix} &amp; \begin{matrix} 0.20 \\ 0.30 \\ 0.30 \\ 0.20 \end{matrix} \end{array}\]</span></p>
  <p>Which horse is the most predictable? One quantitative approach to this question is to look at <em>compressibility</em>. Write down the history of each horse as a string of <span class="math inline">\(200\)</span> values (<span class="math inline">\(\texttt{first, second, third, other}\)</span>). The total number of bits needed to encode these track-record strings can then be computed using Huffman’s algorithm. This works out to <span class="math inline">\(290\)</span> bits for Aurora, <span class="math inline">\(380\)</span> for Whirlwind, and <span class="math inline">\(420\)</span> for Phantasm (check it!). Aurora has the shortest encoding and is therefore in a strong sense the most predictable.</p>
  <p>The inherent unpredictability, or <em>randomness</em>, of a probability distribution can be measured by the extent to which it is possible to compress data drawn from that distribution. <span class="math display">\[\text{more compressible} \equiv \text{less random} \equiv \text{more predictable}\]</span></p>
  <p>Suppose there are <span class="math inline">\(n\)</span> possible outcomes, with probabilities <span class="math inline">\(p_1, p_2, \ldots, p_n\)</span>. If a sequence of <span class="math inline">\(m_4\)</span> values is drawn from the distribution, then the <span class="math inline">\(i\)</span>th outcome will pop up roughly <span class="math inline">\(m p_i\)</span> times (if <span class="math inline">\(m\)</span> is large). For simplicity, assume these are exactly the observed frequencies, and moreover that the <span class="math inline">\(p_i\)</span>'s are all powers of <span class="math inline">\(2\)</span> (that is, of the form <span class="math inline">\(1 / 2^k\)</span>). It can be 􏰑seen by induction (Exercise 5.19) that the number of bits needed to encode the sequence is <span class="math display">\[\sum_{i = 1}^n m p_i \log{\frac{1}{p_i}}.\]</span> Thus the average number of bits needed to encode a single draw from the distribution is <span class="math display">\[\sum_{i = 1}^n p_i \log{\frac{1}{p_i}}.\]</span></p>
  <p>This is the <a href="https://en.wikipedia.org/wiki/Entropy_(information_theory)">entropy</a> of the distribution, a measure of how much randomness it contains.</p>
  <p>For example, a fair coin has two outcomes, each with probability <span class="math inline">\(1 / 2\)</span>. So its entropy is <span class="math display">\[\frac{1}{2} \log{2} + \frac{1}{2} \log{2} = 1.\]</span></p>
  <p>This is natural enough: the coin flip contains one bit of randomness. But what if the coin is not fair, if it has a 3/4 chance of turning up heads? Then the entropy is <span class="math display">\[\frac{3}{4} \log{\frac{4}{3}} + \frac{1}{4} \log{4} = 0.81.\]</span></p>
  <p>A biased coin is more predictable than a fair coin, and thus has lower entropy. As the bias becomes more pronounced, the entropy drops toward zero.</p>
  <p>We explore these notions further in Exercise 5.18 and 5.19.</p>
  </blockquote>
  <div class="footnotes">
  <hr />
  <ol>
  <li id="fn1"><p>For stereo sound, two channels would be needed, doubling the number of samples.<a href="#fnref1">↩</a></p></li>
  </ol>
  </div>

  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
