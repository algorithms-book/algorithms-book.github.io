<!DOCTYPE html>
<html>
<head>

  <title>9.1 Intelligent Exhaustive Search</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta charset="UTF-8">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js"></script>

  <link href="../github-markdown.css" rel="stylesheet" type="text/css"/>
  <style>
      .markdown-body {
          box-sizing: border-box;
          min-width: 200px;
          max-width: 980px;
          margin: 0 auto;
          padding: 45px;
      }

      @media (max-width: 767px) {
          .markdown-body {
              padding: 15px;
          }
      }
  </style>
  <link rel="stylesheet" href="../highlight/styles/atom-one-light.css">

  <script src="../highlight/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>

</head>
<body class="markdown-body">

  <h2 id="intelligent-exhaustive-search">9.1 Intelligent Exhaustive Search</h2>
  <p> </p>
  <h3 id="backtracking">9.1.1 Backtracking</h3>
  <p>Backtracking is based on the observation that it is often possible to reject a solution by looking at just a small portion of it. For example, if an instance of SAT contains the clause (<span class="math inline">\(x_1 \vee x_2\)</span>), then all assignments with <span class="math inline">\(x_1 = x_2 = 0\)</span> (i.e., <span class="math inline">\(\texttt{false}\)</span>) can be instantly eliminated. To put it differently, by quickly checking and discrediting this <em>partial assignment</em>, we are able to prune a quarter of the entire search space. A promising direction, but can it be systematically exploited?</p>
  <p>Here's how it is done. Consider the Boolean formula <span class="math inline">\(\phi(w, x, y, z)\)</span> specified by the set of clauses</p>
  <p><span class="math display">\[(w \vee x \vee y \vee z), (w \vee \bar{x}), (y \vee \bar{z}), (z \vee \bar{w}), (\bar{w} \vee \bar{z}).\]</span></p>
  <p>We will incrementally grow a tree of partial solutions. We start by branching on any one variable, say <span class="math inline">\(w\)</span>: <img src="backtracking-example-w-branch.png" /></p>
  <p>Plugging <span class="math inline">\(w = 0\)</span> and <span class="math inline">\(w = 1\)</span> into <span class="math inline">\(\phi\)</span>, we find that no clause is immediately violated and thus neither of these two partial assignments can be eliminated outright. So we need to keep branching. We can expand either of the two available nodes, and on any variable of our choice. Let's try this one: <img src="backtracking-example-x-branch.png" /></p>
  <p>This time, we are in luck. The partial assignment <span class="math inline">\(w = 0, x = 1\)</span> violates the clause (<span class="math inline">\(w \vee x\)</span>) and can be terminated, thereby pruning a good chunk of the search space. We backtrack out of this cul-de-sac and continue our explorations at one of the two remaining active nodes.</p>
  <p>In this manner, backtracking explores the space of assignments, growing the tree only at nodes where there is uncertainty about the outcome, and stopping if at any stage a satisfying assignment is encountered.</p>
  <p>In the case of Boolean satisfiability, each node of the search tree can be described either by a partial assignment or by the clauses that remain when those values are plugged into the original formula. For instance, if <span class="math inline">\(w = 0\)</span> and <span class="math inline">\(x = 0\)</span> then any clause with <span class="math inline">\(w\)</span> or <span class="math inline">\(x\)</span> is instantly satisfied and any literal <span class="math inline">\(w\)</span> or <span class="math inline">\(x\)</span> is not satisfied and can be removed.</p>
  <p>What's left is</p>
  <p><span class="math display">\[(y \vee z), (\bar{y}), (y \vee \bar{z}).\]</span></p>
  <p>Likewise, <span class="math inline">\(w = 0\)</span> and <span class="math inline">\(x = 1\)</span> leaves</p>
  <p><span class="math display">\[(), (y \vee \bar{z}),\]</span></p>
  <p>with the “empty clause” <span class="math inline">\(()\)</span> ruling out satisfiability. Thus the nodes of the search tree, representing partial assignments, are themselves SAT <em>subproblems</em>.</p>
  <p>This alternative representation is helpful for making the two decisions that repeatedly arise: which subproblem to expand next, and which branching variable to use. Since the benefit of backtracking lies in its ability to eliminate portions of the search space, and since this happens only when an empty clause is encountered, it makes sense to choose the subproblem that contains the <em>smallest clause</em> and to then branch on a variable in that clause.</p>
  <p>If this clause happens to be a singleton, then at least one of the resulting branches will be terminated. (If there is a tie in choosing subproblems, one reasonable policy is to pick the one lowest in the tree, in the hope that it is close to a satisfying assignment.) See Figure 9.1 for the conclusion of our earlier example.</p>
  <div class="figure">
  <img src="fig-9.1-backtracking-example.png" alt="Figure 9.1 Backtracking reveals that \phi is not satisfiable." />
  <p class="caption"><strong>Figure 9.1</strong> Backtracking reveals that <span class="math inline">\(\phi\)</span> is not satisfiable.</p>
  </div>
  <p>More abstractly, a backtracking algorithm requires a <em>test</em> that looks at a subproblem and quickly declares one of three outcomes:</p>
  <ol style="list-style-type: decimal">
  <li><p>Failure: the subproblem has no solution.</p></li>
  <li><p>Success: a solution to the subproblem is found.</p></li>
  <li><p>Uncertainty.</p></li>
  </ol>
  <p>In the case of SAT, this test declares failure if there is an empty clause, success if there are no clauses, and uncertainty otherwise. The backtracking procedure then has the following format.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Start <span class="cf">with</span> some problem P0
  Let S <span class="op">=</span> {P_0}, the <span class="bu">set</span> of active subproblems

  Repeat <span class="cf">while</span> S <span class="kw">is</span> nonempty:
    choose a subproblem P ∈ S <span class="kw">and</span> remove it <span class="im">from</span> S
    expand it into smaller subproblems P_1, P_2, ..., P_k

    For each P_i:
      If test(P_i) succeeds: halt <span class="kw">and</span> announce this solution
      If test(P_i) fails: discard P_i
      Otherwise: add P_i to S

  Announce that there <span class="kw">is</span> no solution</code></pre></div>
  <p>For SAT, the choose procedure picks a clause, and <span class="math inline">\(\texttt{expand}\)</span> picks a variable within that clause. We have already discussed some reasonable ways of making such choices.</p>
  <p>With the right <span class="math inline">\(\texttt{test}, \texttt{expand},\)</span> and <span class="math inline">\(\texttt{choose}\)</span> routines, backtracking can be remarkably effective in practice. The backtracking algorithm we showed for SAT is the basis of many successful satisfiability programs. Another sign of quality is this: if presented with a 2SAT instance, it will always find a satisfying assignment, if one exists, in polynomial time (Exercise 9.1)!</p>
  <p> </p>
  <h3 id="branch-and-bound">9.1.2 Branch-and-Bound</h3>
  <p>The same principle can be generalized from search problems such as SAT to optimization problems. For concreteness, let's say we have a minimization problem; maximization will follow the same pattern.</p>
  <p>As before, we will deal with partial solutions, each of which represents a <em>subproblem</em>, namely, what is the (cost of the) best way to complete this solution? And as before, we need a basis for eliminating partial solutions, since there is no other source of efficiency in our method. To reject a subproblem, we must be certain that its cost exceeds that of some other solution we have already encountered. But its exact cost is unknown to us and is generally not efficiently computable. So instead we use a quick <em>lower bound</em> on this cost.</p>
  <div class="sourceCode"><pre class="sourceCode python"><code class="sourceCode python">Start <span class="cf">with</span> some problem P_0
  Let S <span class="op">=</span> {P0}, the <span class="bu">set</span> of active subproblems
  best_so_far <span class="op">=</span> ∞

  Repeat <span class="cf">while</span> S <span class="kw">is</span> nonempty:
    choose a subproblem (partial solution) P ∈ S <span class="kw">and</span> remove it <span class="im">from</span> S expand it into smaller subproblems P_1, P_2, ..., P_k

    For each P_i:
      If P_i <span class="kw">is</span> a complete solution: update bestsofar
      <span class="cf">else</span> <span class="cf">if</span> lowerbound(Pi) <span class="op">&lt;</span> best_so_far: add P_i to S

  <span class="cf">return</span> best_so_far</code></pre></div>
  <p>Let's see how this works for the traveling salesman problem on a graph <span class="math inline">\(G = (V, E)\)</span> with edge lengths <span class="math inline">\(d_e &gt; 0\)</span>. A partial solution is a simple path <span class="math inline">\(a \leadsto b\)</span> passing through some vertices <span class="math inline">\(S \subseteq V\)</span>, where <span class="math inline">\(S\)</span> includes the endpoints <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. We can denote such a partial solution by the tuple <span class="math inline">\([a, S, b]\)</span> – in fact, <span class="math inline">\(a\)</span> will be fixed throughout the algorithm. The corresponding subproblem is to find the best completion of the tour, that is, the cheapest complementary path <span class="math inline">\(b \leadsto a\)</span> with intermediate nodes <span class="math inline">\(V - S\)</span>. Notice that the initial problem is of the form <span class="math inline">\([a, \{a\}, a]\)</span> for any <span class="math inline">\(a \in V\)</span> of our choosing.</p>
  <p>At each step of the branch-and-bound algorithm, we extend a particular partial solution <span class="math inline">\([a, S, b]\)</span> by a single edge (<span class="math inline">\(b, x\)</span>), where <span class="math inline">\(x \in V - S\)</span>. There can be up to <span class="math inline">\(|V - S|\)</span> ways to do this, and each of these branches leads to a subproblem of the form <span class="math inline">\([a, S \cup \{x\}, x]\)</span>.</p>
  <p>How can we lower-bound the cost of completing a partial tour <span class="math inline">\([a, S, b]\)</span>? Many sophisticated methods have been developed for this, but let's look at a rather simple one. The remainder of the tour consists of a path through <span class="math inline">\(V - S\)</span>, plus edges from <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to <span class="math inline">\(V - S\)</span>. Therefore, its cost is at least the sum of the following:</p>
  <ol style="list-style-type: decimal">
  <li><p>The lightest edge from <span class="math inline">\(a\)</span> to <span class="math inline">\(V - S\)</span>.</p></li>
  <li><p>The lightest edge from <span class="math inline">\(b\)</span> to <span class="math inline">\(V - S\)</span>.</p></li>
  <li><p>The minimum spanning tree of <span class="math inline">\(V - S\)</span>.</p></li>
  </ol>
  <p>(Do you see why?) And this lower bound can be computed quickly by a minimum spanning tree algorithm. Figure 9.2 runs through an example: each node of the tree represents a partial tour (specifically, the path from the root to that node) that at some stage is considered by the branch-and-bound procedure. Notice how just <span class="math inline">\(28\)</span> partial solutions are considered, instead of the <span class="math inline">\(7! = 5,040\)</span> that would arise in a brute-force search.</p>
  <div class="figure">
  <img src="fig-9.2-branch-and-bound-example.png" alt="Figure 9.2 (a) A graph and its optimal traveling salesman tour. (b) The branch-and-bound search tree, explored left to right. Boxed numbers indicate lower bounds on cost." />
  <p class="caption"><strong>Figure 9.2</strong> (a) A graph and its optimal traveling salesman tour. (b) The branch-and-bound search tree, explored left to right. Boxed numbers indicate lower bounds on cost.</p>
  </div>

  <script>
    renderMathInElement(
        document.body,
        {
            delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\\[", right: "\\]", display: true},
                {left: "$", right: "$", display: false},
                {left: "\\(", right: "\\)", display: false}
            ]
        }
    );
  </script>
</body>

</html>
